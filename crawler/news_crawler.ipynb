{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16395c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3832a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_list(start, end):\n",
    "    date_list = []\n",
    "    curr = start\n",
    "    while curr <= end:\n",
    "        date_list.append(curr.strftime(\"%Y.%m.%d\"))\n",
    "        curr += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbf73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(json_data):\n",
    "    urls = []\n",
    "    for item in json_data.get('collection', []):\n",
    "        html_str = item.get('html', '')\n",
    "        soup = BeautifulSoup(html_str, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            if 'n.news.naver.com' in link['href']:\n",
    "                urls.append(link['href'])\n",
    "    unique_urls = list(set(urls))\n",
    "    return unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81a5692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_bydate(date, office_id):\n",
    "    collected_urls = []\n",
    "    base_url = \"https://s.search.naver.com/p/newssearch/3/api/tab/more\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0', 'Referer': 'https://search.naver.com/'}\n",
    "    for start in range(1, 2000, 10):\n",
    "        params = {\n",
    "        'abt': 'null',\n",
    "        'de': date,\n",
    "        'ds': date,\n",
    "        'field': '0',\n",
    "        'is_dts': '0',\n",
    "        'is_sug_officeid': '0',\n",
    "        'mynews': '1',\n",
    "        'news_office_checked': office_id,\n",
    "        'nqx_theme': '{\"theme\":{\"sub\":[{\"name\":\"finance\"}]}}',\n",
    "        'nso': f'so:r,p:from{date.replace(\".\",\"\")}to{date.replace(\".\",\"\")},a:all',\n",
    "        'office_category': '0',\n",
    "        'office_section_code': '3',\n",
    "        'office_type': '1',\n",
    "        'pd': '3',\n",
    "        'photo': '0',\n",
    "        'query': '금리',\n",
    "        'rev': '0',\n",
    "        'service_area': '0',\n",
    "        'sm': 'tab_smr',\n",
    "        'sort': '2',\n",
    "        'spq': '0',\n",
    "        'ssc': 'tab.news.all',\n",
    "        'start': start\n",
    "        }\n",
    "        try:\n",
    "            res = requests.get(base_url, headers=headers, params=params, timeout=10)\n",
    "            if res.status_code != 200 or not res.text.strip():\n",
    "                print(f\"[{date}] 수집 종료\")\n",
    "                break\n",
    "            data = res.json()\n",
    "            page_urls = extract_urls(data)\n",
    "            \n",
    "            if not page_urls:\n",
    "                break\n",
    "            \n",
    "            collected_urls.extend(page_urls)\n",
    "            \n",
    "            time.sleep(random.uniform(0.3, 0.6))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{date}] 에러 발생: {e}\")\n",
    "            raise e\n",
    "            \n",
    "    return list(set(collected_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327ff0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "offices = {\"매일경제\": \"1009\", \"한국경제\": \"1015\", \"머니투데이\": \"1008\"}\n",
    "keyword = \"금리\"\n",
    "s_dt = datetime(2005, 5, 1) \n",
    "e_dt = datetime(2005, 5, 30)\n",
    "max_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b2112c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 30일치 수집을 시작합니다.\n",
      "\n",
      "[진행도: 1/30] 2005.05.01 수집 중...\n",
      "2005.05.01 저장 완료(8개 추가됨)\n",
      "\n",
      "[진행도: 2/30] 2005.05.02 수집 중...\n",
      "2005.05.02 저장 완료(8개 추가됨)\n",
      "\n",
      "[진행도: 3/30] 2005.05.03 수집 중...\n",
      "2005.05.03 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 4/30] 2005.05.04 수집 중...\n",
      "2005.05.04 저장 완료(8개 추가됨)\n",
      "\n",
      "[진행도: 5/30] 2005.05.05 수집 중...\n",
      "2005.05.05 저장 완료(3개 추가됨)\n",
      "\n",
      "[진행도: 6/30] 2005.05.06 수집 중...\n",
      "2005.05.06 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 7/30] 2005.05.07 수집 중...\n",
      "2005.05.07는 검색 결과 없음. 다음 날로 넘어갑니다.\n",
      "\n",
      "[진행도: 8/30] 2005.05.08 수집 중...\n",
      "2005.05.08 저장 완료(5개 추가됨)\n",
      "\n",
      "[진행도: 9/30] 2005.05.09 수집 중...\n",
      "2005.05.09 저장 완료(13개 추가됨)\n",
      "\n",
      "[진행도: 10/30] 2005.05.10 수집 중...\n",
      "2005.05.10 저장 완료(7개 추가됨)\n",
      "\n",
      "[진행도: 11/30] 2005.05.11 수집 중...\n",
      "2005.05.11 저장 완료(12개 추가됨)\n",
      "\n",
      "[진행도: 12/30] 2005.05.12 수집 중...\n",
      "2005.05.12 저장 완료(10개 추가됨)\n",
      "\n",
      "[진행도: 13/30] 2005.05.13 수집 중...\n",
      "2005.05.13 저장 완료(3개 추가됨)\n",
      "\n",
      "[진행도: 14/30] 2005.05.14 수집 중...\n",
      "2005.05.14 저장 완료(1개 추가됨)\n",
      "\n",
      "[진행도: 15/30] 2005.05.15 수집 중...\n",
      "2005.05.15 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 16/30] 2005.05.16 수집 중...\n",
      "2005.05.16 저장 완료(8개 추가됨)\n",
      "\n",
      "[진행도: 17/30] 2005.05.17 수집 중...\n",
      "2005.05.17 저장 완료(9개 추가됨)\n",
      "\n",
      "[진행도: 18/30] 2005.05.18 수집 중...\n",
      "2005.05.18 저장 완료(7개 추가됨)\n",
      "\n",
      "[진행도: 19/30] 2005.05.19 수집 중...\n",
      "2005.05.19 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 20/30] 2005.05.20 수집 중...\n",
      "2005.05.20 저장 완료(3개 추가됨)\n",
      "\n",
      "[진행도: 21/30] 2005.05.21 수집 중...\n",
      "2005.05.21 저장 완료(1개 추가됨)\n",
      "\n",
      "[진행도: 22/30] 2005.05.22 수집 중...\n",
      "2005.05.22 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 23/30] 2005.05.23 수집 중...\n",
      "2005.05.23 저장 완료(3개 추가됨)\n",
      "\n",
      "[진행도: 24/30] 2005.05.24 수집 중...\n",
      "2005.05.24 저장 완료(13개 추가됨)\n",
      "\n",
      "[진행도: 25/30] 2005.05.25 수집 중...\n",
      "2005.05.25 저장 완료(6개 추가됨)\n",
      "\n",
      "[진행도: 26/30] 2005.05.26 수집 중...\n",
      "2005.05.26 저장 완료(5개 추가됨)\n",
      "\n",
      "[진행도: 27/30] 2005.05.27 수집 중...\n",
      "2005.05.27 저장 완료(5개 추가됨)\n",
      "\n",
      "[진행도: 28/30] 2005.05.28 수집 중...\n",
      "2005.05.28 저장 완료(1개 추가됨)\n",
      "\n",
      "[진행도: 29/30] 2005.05.29 수집 중...\n",
      "2005.05.29 저장 완료(5개 추가됨)\n",
      "\n",
      "[진행도: 30/30] 2005.05.30 수집 중...\n",
      "2005.05.30 저장 완료(5개 추가됨)\n",
      "\n",
      "모든 수집 완료. 'news_urls_1009.csv' 파일을 확인해 보세요.\n"
     ]
    }
   ],
   "source": [
    "office_id = offices['매일경제']\n",
    "file_name = f'news_urls_{office_id}.csv'\n",
    "target_dates = generate_date_list(s_dt, e_dt)\n",
    "print(f\"총 {len(target_dates)}일치 수집을 시작합니다.\")\n",
    "\n",
    "for i, date in enumerate(target_dates):\n",
    "    print(f\"\\n[진행도: {i+1}/{len(target_dates)}] {date} 수집 중...\")\n",
    "    urls = get_urls_bydate(date, office_id)\n",
    "\n",
    "    if urls:\n",
    "        df = pd.DataFrame(urls, columns=['url'])\n",
    "        df['date'] = date\n",
    "        if not os.path.exists(file_name):\n",
    "            df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "        else:\n",
    "            df.to_csv(file_name, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "        print(f\"{date} 저장 완료({len(urls)}개 추가됨)\")\n",
    "    else:\n",
    "        print(f\"{date}는 검색 결과 없음. 다음 날로 넘어갑니다.\")\n",
    "\n",
    "print(f\"\\n모든 수집 완료. '{file_name}' 파일을 확인해 보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85264674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정리 전: 570개 -> 정리 후: 212개\n"
     ]
    }
   ],
   "source": [
    "# 수집 후 겹치는 url 지우기\n",
    "df = pd.read_csv(f'news_urls_{office_id}.csv')\n",
    "df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "df_clean.to_csv(f\"news_urls_{office_id}_final.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"정리 전: {len(df)}개 -> 정리 후: {len(df_clean)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc9875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '[금융 신상품] 국고채 年 3.70~3.85%',\n",
       " 'date': '2005-05-01 13:17:00',\n",
       " 'content': '지난주 금리는 하락세를 보였다. 5월 국채발행 계획, 3월 산업활동 결과가 시 장 예상 수준을 벗어나지 않았고 미국 1분기 GDP 성장률이 부진하면서 소프트 패치 논쟁이 가열됐기 때문이다. 3월 산업활동 동향은 분명 국내 경기가 저점을 지나고 있음을 확인시켜 주고 있다. 그러나 지표개선 속도가 완만하며 미국 경제지표 부진은 2분기 이후 국 내 경기 불확실성을 키우는 요인이다. 글로벌 경기 둔화 염려가 부각되는 상황 에서 위안화 절상 가능성에 따른 원화 절상 압력은 경기 측면에서 금리하락 요 인으로 해석될 가능성이 높다. 물론 금리 수준 부담으로 금리 추가 하락 여지는 좁아 보이지만 금리 상승 압 력 역시 미약할 것 같다. 이번주 지표금리(3년만기 국고채 수익률)는 연 3.70~ 3.85%에서 움직일 전망이다. [김형기 대우증권 선임연구원] < Copyright ⓒ 매일경제. 무단전재 및 재배포 금지 >',\n",
       " 'url': 'https://n.news.naver.com/mnews/article/009/0000437454?sid=101'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 매일경제 뉴스 수집\n",
    "def mail_news_detail(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0',\n",
    "        'Referer': 'https://news.naver.com/'\n",
    "    }\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=15)\n",
    "        if res.status_code != 200:\n",
    "            return None\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        title = soup.select_one(\"h2#title_area\")\n",
    "        title = title.get_text(strip=True) if title else \"제목 없음\"\n",
    "        \n",
    "        date_tag = soup.select_one(\"span.media_end_head_info_datestamp_time\")\n",
    "        if date_tag and date_tag.has_attr('data-date-time'):\n",
    "            date_str = date_tag['data-date-time']\n",
    "        else:\n",
    "            date_str = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "        content = soup.select_one(\"#newsct_article\") # 또는 \"#dic_area\"\n",
    "        \n",
    "        if content:\n",
    "            content = content.get_text(\" \", strip=True)\n",
    "        else:\n",
    "            content = \"본문 없음\"\n",
    "            \n",
    "        return {'title': title, 'date': date_str, 'content': content, 'url': url}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[{url}] 수집 중 에러 발생: {e}\") \n",
    "        raise e\n",
    "\n",
    "news_detail('https://n.news.naver.com/mnews/article/009/0000437454?sid=101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a9e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
