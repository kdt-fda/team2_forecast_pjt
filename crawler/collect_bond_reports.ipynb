{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63d0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.32.5)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.22.4; python_version < \"3.11\" in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20987971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.36.0)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: urllib3[socks]<3.0,>=2.5.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from webdriver-manager) (25.0)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome>=1.2.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: idna in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.11)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\"\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: pycparser; implementation_name != \"PyPy\" in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio<1.0,>=0.30.0->selenium) (2.23)\n",
      "Installing collected packages: python-dotenv, webdriver-manager, pysocks\n",
      "Successfully installed pysocks-1.7.1 python-dotenv-1.2.1 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 20.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba5b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "최종 추출 건수: 0건\n",
      "==================================================\n",
      "여전히 데이터가 0개라면 클래스명 외에 다른 구조도 확인이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. 타겟 URL 및 헤더 설정\n",
    "url = \"https://finance.naver.com/research/debenture_list.naver\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://finance.naver.com/research/debenture_list.naver'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 2. 페이지 요청\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'euc-kr'\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # 3. 데이터 추출 (강사님 피드백: 클래스명을 'type_1'로 수정)\n",
    "    # 강사님이 이미지에서 짚어주신대로 table.type_1 내부의 tr을 찾습니다.\n",
    "    rows = soup.select('table.type_1 tr')\n",
    "    \n",
    "    bond_data = []\n",
    "    \n",
    "    for row in rows:\n",
    "        tds = row.find_all('td')\n",
    "        # 데이터가 있는 행은 보통 td가 5개입니다.\n",
    "        if len(tds) >= 4:\n",
    "            company = tds[0].text.strip() # 증권사\n",
    "            title_tag = tds[1].find('a')  # 제목 태그\n",
    "            \n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                # 상세 페이지 링크 (PDF를 따기 위해 필요)\n",
    "                link = \"https://finance.naver.com/research/\" + title_tag['href']\n",
    "                date = tds[4].text.strip() # 작성일\n",
    "                \n",
    "                bond_data.append([date, company, title, link])\n",
    "                print(f\"[성공] {date} | {company} | {title[:15]}...\")\n",
    "\n",
    "    # 4. 결과 확인\n",
    "    df = pd.DataFrame(bond_data, columns=['Date', 'Company', 'Title', 'Link'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"최종 추출 건수: {len(df)}건\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(df.head(10))\n",
    "    else:\n",
    "        print(\"여전히 데이터가 0개라면 클래스명 외에 다른 구조도 확인이 필요합니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"에러 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942dd3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 추출 결과: 30건 ---\n",
      "    0       1                                     2  \\\n",
      "0  77    하나증권          성장률 컨센서스와 금리(’14, ’17, ’26년)   \n",
      "1  52  다올투자증권  Daily Bond Morning Brief(2025.12.30)   \n",
      "2  42  다올투자증권  Daily Bond Morning Brief(2025.12.29)   \n",
      "3  55   유안타증권                      채권 Daily (12.30)   \n",
      "4  50  유진투자증권                   Eugenes FICC Update   \n",
      "\n",
      "                                                   3  \n",
      "0  https://finance.naver.com/research/debenture_r...  \n",
      "1  https://finance.naver.com/research/debenture_r...  \n",
      "2  https://finance.naver.com/research/debenture_r...  \n",
      "3  https://finance.naver.com/research/debenture_r...  \n",
      "4  https://finance.naver.com/research/debenture_r...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 세션(Session)을 생성하여 쿠키를 자동으로 관리하게 합니다.\n",
    "session = requests.Session()\n",
    "\n",
    "url = \"https://finance.naver.com/research/debenture_list.naver\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://finance.naver.com/research/debenture_list.naver',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 세션을 통해 첫 접속을 시도하여 쿠키를 굽습니다.\n",
    "    res = session.get(url, headers=headers)\n",
    "    res.encoding = 'euc-kr'\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # 강사님이 짚어준 필살 셀렉터\n",
    "    rows = soup.select('table.type_1 tr')\n",
    "    \n",
    "    bond_data = []\n",
    "    for row in rows:\n",
    "        tds = row.find_all('td')\n",
    "        # 네이버 채권 리스트는 td가 보통 6개입니다. (번호, 제목, 증권사, 조회수, 날짜 등)\n",
    "        if len(tds) >= 4:\n",
    "            # 강사님 이미지 구조대로 인덱스를 재조정합니다.\n",
    "            title_tag = tds[0].find('a') # 보통 첫 번째나 두 번째에 제목이 있습니다.\n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                company = tds[1].text.strip()\n",
    "                date = tds[4].text.strip()\n",
    "                link = \"https://finance.naver.com/research/\" + title_tag['href']\n",
    "                bond_data.append([date, company, title, link])\n",
    "\n",
    "    df = pd.DataFrame(bond_data)\n",
    "    print(f\"--- 추출 결과: {len(df)}건 ---\")\n",
    "    if not df.empty:\n",
    "        print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"에러: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2378fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 30개의 PDF를 정제 후 다운로드합니다...\n",
      "\n",
      "--- 모든 오류를 잡고 다운로드를 완료했습니다! ---\n",
      "    0       1                                     2  \\\n",
      "0  77    하나증권          성장률 컨센서스와 금리(’14, ’17, ’26년)   \n",
      "1  52  다올투자증권  Daily Bond Morning Brief(2025.12.30)   \n",
      "2  42  다올투자증권  Daily Bond Morning Brief(2025.12.29)   \n",
      "3  55   유안타증권                      채권 Daily (12.30)   \n",
      "4  50  유진투자증권                   Eugenes FICC Update   \n",
      "\n",
      "                                                   3  \n",
      "0  https://finance.naver.com/research/debenture_r...  \n",
      "1  https://finance.naver.com/research/debenture_r...  \n",
      "2  https://finance.naver.com/research/debenture_r...  \n",
      "3  https://finance.naver.com/research/debenture_r...  \n",
      "4  https://finance.naver.com/research/debenture_r...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# 1. 저장 폴더 설정\n",
    "save_dir = \"bond_reports\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(f\"총 {len(bond_data)}개의 PDF를 정제 후 다운로드합니다...\")\n",
    "\n",
    "for i, report in enumerate(bond_data):\n",
    "    # report 구조: [0:조회수, 1:증권사, 2:제목, 3:상세링크]\n",
    "    # (주의: 아까 결과에서 컬럼 순서가 다를 수 있으니 인덱스 확인!)\n",
    "    \n",
    "    title_raw = report[2] \n",
    "    company = report[1]\n",
    "    detail_url = report[3]\n",
    "    \n",
    "    try:\n",
    "        # 상세 페이지 접속\n",
    "        res = session.get(detail_url, headers=headers)\n",
    "        res.encoding = 'euc-kr'\n",
    "        detail_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        pdf_btn = detail_soup.select_one('a.file_down')\n",
    "        if pdf_btn:\n",
    "            pdf_url = pdf_btn['href']\n",
    "            \n",
    "            # [오류 해결 포인트] 제목 내 특수문자('14, '17 등)를 안전한 문자로 변경\n",
    "            # 파일명에 쓸 수 없는 문자나 홑따옴표를 언더바(_)나 공백으로 치환합니다.\n",
    "            clean_title = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"_\", title_raw)\n",
    "            clean_title = clean_title.replace(\"__\", \"_\").strip() # 연속된 언더바 정리\n",
    "            \n",
    "            file_name = f\"{save_dir}/{company}_{clean_title[:40]}.pdf\"\n",
    "            \n",
    "            # PDF 다운로드\n",
    "            urllib.request.urlretrieve(pdf_url, file_name)\n",
    "            print(f\"[{i+1}] 저장 성공: {file_name}\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{i+1}] 실패: {title_raw[:20]}... (에러: {e})\")\n",
    "\n",
    "print(\"\\n--- 모든 오류를 잡고 다운로드를 완료했습니다! ---\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d31b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 30개의 리포트 분석 및 다운로드 시작...\n",
      "[0] PDF 링크를 찾을 수 없음: 성장률 컨센서스와 금리(’1\n",
      "[1] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[2] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[3] PDF 링크를 찾을 수 없음: 채권 Daily (12.30\n",
      "[4] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[5] PDF 링크를 찾을 수 없음: [FI Monthly] Ca\n",
      "[6] PDF 링크를 찾을 수 없음: KP: 1월, 쏟아지는 공급\n",
      "[7] PDF 링크를 찾을 수 없음: 채권 Daily (12.29\n",
      "[8] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[9] PDF 링크를 찾을 수 없음: 환율은 진정되는 듯하지만..\n",
      "[10] PDF 링크를 찾을 수 없음: 채권 Daily (12.26\n",
      "[11] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[12] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[13] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[14] PDF 링크를 찾을 수 없음: 최근 신용스프레드 하방경직성\n",
      "[15] PDF 링크를 찾을 수 없음: 채권 Daily (12.24\n",
      "[16] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[17] PDF 링크를 찾을 수 없음: 채권 Daily (12.23\n",
      "[18] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[19] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[20] PDF 링크를 찾을 수 없음: Weekly Credit M\n",
      "[21] PDF 링크를 찾을 수 없음: [FI Weekly] 상반기\n",
      "[22] PDF 링크를 찾을 수 없음: [12월 BOJ] 중립금리를\n",
      "[23] PDF 링크를 찾을 수 없음: 채권시장이 바라는 연준 의장\n",
      "[24] PDF 링크를 찾을 수 없음: 채권 Daily (12.22\n",
      "[25] PDF 링크를 찾을 수 없음: 12/22 Kiwoom Fi\n",
      "[26] PDF 링크를 찾을 수 없음: Daily Bond Morn\n",
      "[27] PDF 링크를 찾을 수 없음: Eugenes FICC Up\n",
      "[28] PDF 링크를 찾을 수 없음: 2025년 미국 CLO 시장\n",
      "[29] PDF 링크를 찾을 수 없음: 채권 Daily (12.19\n",
      "\n",
      "--- 모든 시도가 완료되었습니다. 폴더를 확인하세요! ---\n",
      "    0       1                                     2  \\\n",
      "0  77    하나증권          성장률 컨센서스와 금리(’14, ’17, ’26년)   \n",
      "1  52  다올투자증권  Daily Bond Morning Brief(2025.12.30)   \n",
      "2  42  다올투자증권  Daily Bond Morning Brief(2025.12.29)   \n",
      "3  55   유안타증권                      채권 Daily (12.30)   \n",
      "4  50  유진투자증권                   Eugenes FICC Update   \n",
      "\n",
      "                                                   3  \n",
      "0  https://finance.naver.com/research/debenture_r...  \n",
      "1  https://finance.naver.com/research/debenture_r...  \n",
      "2  https://finance.naver.com/research/debenture_r...  \n",
      "3  https://finance.naver.com/research/debenture_r...  \n",
      "4  https://finance.naver.com/research/debenture_r...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# 1. 저장 폴더 다시 확인\n",
    "save_dir = \"bond_reports_final\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(f\"총 {len(bond_data)}개의 리포트 분석 및 다운로드 시작...\")\n",
    "\n",
    "for i, report in enumerate(bond_data):\n",
    "    # 데이터 구조에 따라 인덱스를 조정하세요 (현재: 1=증권사, 2=제목, 3=상세링크)\n",
    "    company = report[1]\n",
    "    title_raw = report[2]\n",
    "    detail_url = report[3]\n",
    "    \n",
    "    try:\n",
    "        # A. 상세 페이지 접속 (PDF 주소를 찾기 위함)\n",
    "        res = session.get(detail_url, headers=headers)\n",
    "        res.encoding = 'euc-kr'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # B. PDF 주소 추출\n",
    "        pdf_btn = soup.select_one('a.file_down')\n",
    "        if not pdf_btn:\n",
    "            print(f\"[{i}] PDF 링크를 찾을 수 없음: {title_raw[:15]}\")\n",
    "            continue\n",
    "            \n",
    "        pdf_url = pdf_btn['href']\n",
    "        \n",
    "        # C. [핵심] 파일명 정제 (특수문자 '14, '17 등을 언더바 또는 제거)\n",
    "        # 윈도우 파일명 금지 문자: \\ / : * ? \" < > | 및 따옴표 제거\n",
    "        clean_title = re.sub(r'[\\/:*?\"<>|]', '', title_raw) # 금지 문자 제거\n",
    "        clean_title = clean_title.replace(\"’\", \"\").replace(\"'\", \"\") # 따옴표 제거\n",
    "        file_name = f\"{company}_{clean_title[:30]}.pdf\"\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        \n",
    "        # D. [핵심] PDF 파일 다운로드 (Streaming 방식)\n",
    "        # 단순히 주소를 요청하는 게 아니라 파일 데이터를 쪼개서 받아옵니다.\n",
    "        pdf_res = session.get(pdf_url, headers=headers, stream=True)\n",
    "        \n",
    "        if pdf_res.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in pdf_res.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            print(f\"[{i+1}] 성공: {file_name}\")\n",
    "        else:\n",
    "            print(f\"[{i+1}] 실패: 서버 응답 코드 {pdf_res.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{i+1}] 에러 발생: {str(e)}\")\n",
    "\n",
    "print(\"\\n--- 모든 시도가 완료되었습니다. 폴더를 확인하세요! ---\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c669dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성벽을 넘기 위해 전략을 바꿨습니다. PDF 주소 정밀 추적 시작...\n",
      "[1] 드디어 성공!!: 하나증권_성장률 컨센서스와 금리14 17 26.pdf\n",
      "[2] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[3] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[4] 드디어 성공!!: 유안타증권_채권 Daily 1230.pdf\n",
      "[5] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[6] 드디어 성공!!: 한화투자증권_FI Monthly Carry New.pdf\n",
      "[7] 드디어 성공!!: 하나증권_KP 1월 쏟아지는 공급과 기회의 창.pdf\n",
      "[8] 드디어 성공!!: 유안타증권_채권 Daily 1229.pdf\n",
      "[9] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[9] 여전히 링크 못 찾음: 환율은 진정되는 듯하지만..\n",
      "[11] 드디어 성공!!: 유안타증권_채권 Daily 1226.pdf\n",
      "[12] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[13] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[14] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[15] 드디어 성공!!: iM증권_최근 신용스프레드 하방경직성과 거래대.pdf\n",
      "[16] 드디어 성공!!: 유안타증권_채권 Daily 1224.pdf\n",
      "[17] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[18] 드디어 성공!!: 유안타증권_채권 Daily 1223.pdf\n",
      "[19] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[20] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[20] 여전히 링크 못 찾음: Weekly Credit M\n",
      "[22] 드디어 성공!!: 한화투자증권_FI Weekly 상반기 수급부담 감.pdf\n",
      "[23] 드디어 성공!!: 하나증권_12월 BOJ 중립금리를 향한 여정은.pdf\n",
      "[24] 드디어 성공!!: 하나증권_채권시장이 바라는 연준 의장 Top .pdf\n",
      "[25] 드디어 성공!!: 유안타증권_채권 Daily 1222.pdf\n",
      "[26] 드디어 성공!!: 키움증권_1222 Kiwoom Fixed In.pdf\n",
      "[27] 드디어 성공!!: 다올투자증권_Daily Bond Morning B.pdf\n",
      "[28] 드디어 성공!!: 유진투자증권_Eugenes FICC Update.pdf\n",
      "[29] 드디어 성공!!: iM증권_2025년 미국 CLO 시장 동향 및.pdf\n",
      "[30] 드디어 성공!!: 유안타증권_채권 Daily 1219.pdf\n",
      "\n",
      "작업 완료. 폴더에 파일이 하나라도 생겼는지 확인해주세요!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "save_dir = \"bond_reports_final\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(\"성벽을 넘기 위해 전략을 바꿨습니다. PDF 주소 정밀 추적 시작...\")\n",
    "\n",
    "for i, report in enumerate(bond_data):\n",
    "    company = report[1]\n",
    "    title_raw = report[2]\n",
    "    detail_url = report[3]\n",
    "    \n",
    "    try:\n",
    "        res = session.get(detail_url, headers=headers)\n",
    "        res.encoding = 'euc-kr'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # [핵심 변경] 특정 클래스명이 아니라, href 속성에 '.pdf'가 포함된 모든 a 태그를 찾습니다.\n",
    "        pdf_link = None\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        for link in all_links:\n",
    "            if '.pdf' in link['href'].lower():\n",
    "                pdf_link = link['href']\n",
    "                break\n",
    "        \n",
    "        if not pdf_link:\n",
    "            # 만약 a 태그에 없다면, 버튼(button) 태그 내의 onclick 자바스크립트 등도 확인해봐야 하지만\n",
    "            # 우선 가장 확률 높은 방식으로 시도합니다.\n",
    "            print(f\"[{i}] 여전히 링크 못 찾음: {title_raw[:15]}\")\n",
    "            continue\n",
    "            \n",
    "        # 파일명 정제 (특수문자 제거)\n",
    "        clean_title = re.sub(r'[^\\uAC00-\\uD7A30-9a-zA-Z\\s]', '', title_raw)\n",
    "        file_name = f\"{company}_{clean_title[:20]}.pdf\"\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        \n",
    "        # 다운로드 시도\n",
    "        pdf_res = session.get(pdf_link, headers=headers, stream=True)\n",
    "        if pdf_res.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in pdf_res.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"[{i+1}] 드디어 성공!!: {file_name}\")\n",
    "        else:\n",
    "            print(f\"[{i+1}] 서버 응답 에러: {pdf_res.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{i+1}] 에러: {str(e)}\")\n",
    "\n",
    "print(\"\\n작업 완료. 폴더에 파일이 하나라도 생겼는지 확인해주세요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34058439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지부터 수집을 시작합니다. 대장정의 시작!\n",
      "[Page 1] 30건 수집 완료 (누적: 30건)\n",
      "[Page 2] 30건 수집 완료 (누적: 60건)\n",
      "[Page 3] 30건 수집 완료 (누적: 90건)\n",
      "[Page 4] 30건 수집 완료 (누적: 120건)\n",
      "[Page 5] 30건 수집 완료 (누적: 150건)\n",
      "[Page 6] 30건 수집 완료 (누적: 180건)\n",
      "[Page 7] 30건 수집 완료 (누적: 210건)\n",
      "[Page 8] 30건 수집 완료 (누적: 240건)\n",
      "[Page 9] 30건 수집 완료 (누적: 270건)\n",
      "[Page 10] 30건 수집 완료 (누적: 300건)\n",
      "[Page 11] 30건 수집 완료 (누적: 330건)\n",
      "[Page 12] 30건 수집 완료 (누적: 360건)\n",
      "[Page 13] 30건 수집 완료 (누적: 390건)\n",
      "[Page 14] 30건 수집 완료 (누적: 420건)\n",
      "[Page 15] 30건 수집 완료 (누적: 450건)\n",
      "[Page 16] 30건 수집 완료 (누적: 480건)\n",
      "[Page 17] 30건 수집 완료 (누적: 510건)\n",
      "[Page 18] 30건 수집 완료 (누적: 540건)\n",
      "[Page 19] 30건 수집 완료 (누적: 570건)\n",
      "[Page 20] 30건 수집 완료 (누적: 600건)\n",
      "[Page 21] 30건 수집 완료 (누적: 630건)\n",
      "[Page 22] 30건 수집 완료 (누적: 660건)\n",
      "[Page 23] 30건 수집 완료 (누적: 690건)\n",
      "[Page 24] 30건 수집 완료 (누적: 720건)\n",
      "[Page 25] 30건 수집 완료 (누적: 750건)\n",
      "[Page 26] 30건 수집 완료 (누적: 780건)\n",
      "[Page 27] 30건 수집 완료 (누적: 810건)\n",
      "[Page 28] 30건 수집 완료 (누적: 840건)\n",
      "[Page 29] 30건 수집 완료 (누적: 870건)\n",
      "[Page 30] 30건 수집 완료 (누적: 900건)\n",
      "[Page 31] 30건 수집 완료 (누적: 930건)\n",
      "[Page 32] 30건 수집 완료 (누적: 960건)\n",
      "[Page 33] 30건 수집 완료 (누적: 990건)\n",
      "[Page 34] 30건 수집 완료 (누적: 1020건)\n",
      "[Page 35] 30건 수집 완료 (누적: 1050건)\n",
      "[Page 36] 30건 수집 완료 (누적: 1080건)\n",
      "[Page 37] 30건 수집 완료 (누적: 1110건)\n",
      "[Page 38] 30건 수집 완료 (누적: 1140건)\n",
      "[Page 39] 30건 수집 완료 (누적: 1170건)\n",
      "[Page 40] 30건 수집 완료 (누적: 1200건)\n",
      "[Page 41] 30건 수집 완료 (누적: 1230건)\n",
      "[Page 42] 30건 수집 완료 (누적: 1260건)\n",
      "[Page 43] 30건 수집 완료 (누적: 1290건)\n",
      "[Page 44] 30건 수집 완료 (누적: 1320건)\n",
      "[Page 45] 30건 수집 완료 (누적: 1350건)\n",
      "[Page 46] 30건 수집 완료 (누적: 1380건)\n",
      "[Page 47] 30건 수집 완료 (누적: 1410건)\n",
      "[Page 48] 30건 수집 완료 (누적: 1440건)\n",
      "[Page 49] 30건 수집 완료 (누적: 1470건)\n",
      "[Page 50] 30건 수집 완료 (누적: 1500건)\n",
      "[Page 51] 30건 수집 완료 (누적: 1530건)\n",
      "[Page 52] 30건 수집 완료 (누적: 1560건)\n",
      "[Page 53] 30건 수집 완료 (누적: 1590건)\n",
      "[Page 54] 30건 수집 완료 (누적: 1620건)\n",
      "[Page 55] 30건 수집 완료 (누적: 1650건)\n",
      "[Page 56] 30건 수집 완료 (누적: 1680건)\n",
      "[Page 57] 30건 수집 완료 (누적: 1710건)\n",
      "[Page 58] 30건 수집 완료 (누적: 1740건)\n",
      "[Page 59] 30건 수집 완료 (누적: 1770건)\n",
      "[Page 60] 30건 수집 완료 (누적: 1800건)\n",
      "[Page 61] 30건 수집 완료 (누적: 1830건)\n",
      "[Page 62] 30건 수집 완료 (누적: 1860건)\n",
      "[Page 63] 30건 수집 완료 (누적: 1890건)\n",
      "[Page 64] 30건 수집 완료 (누적: 1920건)\n",
      "[Page 65] 30건 수집 완료 (누적: 1950건)\n",
      "[Page 66] 30건 수집 완료 (누적: 1980건)\n",
      "[Page 67] 30건 수집 완료 (누적: 2010건)\n",
      "[Page 68] 30건 수집 완료 (누적: 2040건)\n",
      "[Page 69] 30건 수집 완료 (누적: 2070건)\n",
      "[Page 70] 30건 수집 완료 (누적: 2100건)\n",
      "[Page 71] 30건 수집 완료 (누적: 2130건)\n",
      "[Page 72] 30건 수집 완료 (누적: 2160건)\n",
      "[Page 73] 30건 수집 완료 (누적: 2190건)\n",
      "[Page 74] 30건 수집 완료 (누적: 2220건)\n",
      "[Page 75] 30건 수집 완료 (누적: 2250건)\n",
      "[Page 76] 30건 수집 완료 (누적: 2280건)\n",
      "[Page 77] 30건 수집 완료 (누적: 2310건)\n",
      "[Page 78] 30건 수집 완료 (누적: 2340건)\n",
      "[Page 79] 30건 수집 완료 (누적: 2370건)\n",
      "[Page 80] 30건 수집 완료 (누적: 2400건)\n",
      "[Page 81] 30건 수집 완료 (누적: 2430건)\n",
      "[Page 82] 30건 수집 완료 (누적: 2460건)\n",
      "[Page 83] 30건 수집 완료 (누적: 2490건)\n",
      "[Page 84] 30건 수집 완료 (누적: 2520건)\n",
      "[Page 85] 30건 수집 완료 (누적: 2550건)\n",
      "[Page 86] 30건 수집 완료 (누적: 2580건)\n",
      "[Page 87] 30건 수집 완료 (누적: 2610건)\n",
      "[Page 88] 30건 수집 완료 (누적: 2640건)\n",
      "[Page 89] 30건 수집 완료 (누적: 2670건)\n",
      "[Page 90] 30건 수집 완료 (누적: 2700건)\n",
      "[Page 91] 30건 수집 완료 (누적: 2730건)\n",
      "[Page 92] 30건 수집 완료 (누적: 2760건)\n",
      "[Page 93] 30건 수집 완료 (누적: 2790건)\n",
      "[Page 94] 30건 수집 완료 (누적: 2820건)\n",
      "[Page 95] 30건 수집 완료 (누적: 2850건)\n",
      "[Page 96] 30건 수집 완료 (누적: 2880건)\n",
      "[Page 97] 30건 수집 완료 (누적: 2910건)\n",
      "[Page 98] 30건 수집 완료 (누적: 2940건)\n",
      "[Page 99] 30건 수집 완료 (누적: 2970건)\n",
      "[Page 100] 30건 수집 완료 (누적: 3000건)\n",
      "[Page 101] 30건 수집 완료 (누적: 3030건)\n",
      "[Page 102] 30건 수집 완료 (누적: 3060건)\n",
      "[Page 103] 30건 수집 완료 (누적: 3090건)\n",
      "[Page 104] 30건 수집 완료 (누적: 3120건)\n",
      "[Page 105] 30건 수집 완료 (누적: 3150건)\n",
      "[Page 106] 30건 수집 완료 (누적: 3180건)\n",
      "[Page 107] 30건 수집 완료 (누적: 3210건)\n",
      "[Page 108] 30건 수집 완료 (누적: 3240건)\n",
      "[Page 109] 30건 수집 완료 (누적: 3270건)\n",
      "[Page 110] 30건 수집 완료 (누적: 3300건)\n",
      "[Page 111] 30건 수집 완료 (누적: 3330건)\n",
      "[Page 112] 30건 수집 완료 (누적: 3360건)\n",
      "[Page 113] 30건 수집 완료 (누적: 3390건)\n",
      "[Page 114] 30건 수집 완료 (누적: 3420건)\n",
      "[Page 115] 30건 수집 완료 (누적: 3450건)\n",
      "[Page 116] 30건 수집 완료 (누적: 3480건)\n",
      "[Page 117] 30건 수집 완료 (누적: 3510건)\n",
      "[Page 118] 30건 수집 완료 (누적: 3540건)\n",
      "[Page 119] 30건 수집 완료 (누적: 3570건)\n",
      "[Page 120] 30건 수집 완료 (누적: 3600건)\n",
      "[Page 121] 30건 수집 완료 (누적: 3630건)\n",
      "[Page 122] 30건 수집 완료 (누적: 3660건)\n",
      "[Page 123] 30건 수집 완료 (누적: 3690건)\n",
      "[Page 124] 30건 수집 완료 (누적: 3720건)\n",
      "[Page 125] 30건 수집 완료 (누적: 3750건)\n",
      "[Page 126] 30건 수집 완료 (누적: 3780건)\n",
      "[Page 127] 30건 수집 완료 (누적: 3810건)\n",
      "[Page 128] 30건 수집 완료 (누적: 3840건)\n",
      "[Page 129] 30건 수집 완료 (누적: 3870건)\n",
      "[Page 130] 30건 수집 완료 (누적: 3900건)\n",
      "[Page 131] 30건 수집 완료 (누적: 3930건)\n",
      "[Page 132] 30건 수집 완료 (누적: 3960건)\n",
      "[Page 133] 30건 수집 완료 (누적: 3990건)\n",
      "[Page 134] 30건 수집 완료 (누적: 4020건)\n",
      "[Page 135] 30건 수집 완료 (누적: 4050건)\n",
      "[Page 136] 30건 수집 완료 (누적: 4080건)\n",
      "[Page 137] 30건 수집 완료 (누적: 4110건)\n",
      "[Page 138] 30건 수집 완료 (누적: 4140건)\n",
      "[Page 139] 30건 수집 완료 (누적: 4170건)\n",
      "[Page 140] 30건 수집 완료 (누적: 4200건)\n",
      "[Page 141] 30건 수집 완료 (누적: 4230건)\n",
      "[Page 142] 30건 수집 완료 (누적: 4260건)\n",
      "[Page 143] 30건 수집 완료 (누적: 4290건)\n",
      "[Page 144] 30건 수집 완료 (누적: 4320건)\n",
      "[Page 145] 30건 수집 완료 (누적: 4350건)\n",
      "[Page 146] 30건 수집 완료 (누적: 4380건)\n",
      "[Page 147] 30건 수집 완료 (누적: 4410건)\n",
      "[Page 148] 30건 수집 완료 (누적: 4440건)\n",
      "[Page 149] 30건 수집 완료 (누적: 4470건)\n",
      "[Page 150] 30건 수집 완료 (누적: 4500건)\n",
      "[Page 151] 30건 수집 완료 (누적: 4530건)\n",
      "[Page 152] 30건 수집 완료 (누적: 4560건)\n",
      "[Page 153] 30건 수집 완료 (누적: 4590건)\n",
      "[Page 154] 30건 수집 완료 (누적: 4620건)\n",
      "[Page 155] 30건 수집 완료 (누적: 4650건)\n",
      "[Page 156] 30건 수집 완료 (누적: 4680건)\n",
      "[Page 157] 30건 수집 완료 (누적: 4710건)\n",
      "[Page 158] 30건 수집 완료 (누적: 4740건)\n",
      "[Page 159] 30건 수집 완료 (누적: 4770건)\n",
      "[Page 160] 30건 수집 완료 (누적: 4800건)\n",
      "[Page 161] 30건 수집 완료 (누적: 4830건)\n",
      "[Page 162] 30건 수집 완료 (누적: 4860건)\n",
      "[Page 163] 30건 수집 완료 (누적: 4890건)\n",
      "[Page 164] 30건 수집 완료 (누적: 4920건)\n",
      "[Page 165] 30건 수집 완료 (누적: 4950건)\n",
      "[Page 166] 에러 발생: ('Connection aborted.', ConnectionAbortedError(10053, '현재 연결은 사용자의 호스트 시스템의 소프트웨어의 의해 중단되었습니다', None, 10053, None))\n",
      "[Page 167] 에러 발생: HTTPSConnectionPool(host='finance.naver.com', port=443): Max retries exceeded with url: /research/debenture_list.naver?page=167 (Caused by NameResolutionError(\"HTTPSConnection(host='finance.naver.com', port=443): Failed to resolve 'finance.naver.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[Page 168] 에러 발생: HTTPSConnectionPool(host='finance.naver.com', port=443): Max retries exceeded with url: /research/debenture_list.naver?page=168 (Caused by NameResolutionError(\"HTTPSConnection(host='finance.naver.com', port=443): Failed to resolve 'finance.naver.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[Page 169] 에러 발생: HTTPSConnectionPool(host='finance.naver.com', port=443): Max retries exceeded with url: /research/debenture_list.naver?page=169 (Caused by NameResolutionError(\"HTTPSConnection(host='finance.naver.com', port=443): Failed to resolve 'finance.naver.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[Page 170] 에러 발생: HTTPSConnectionPool(host='finance.naver.com', port=443): Max retries exceeded with url: /research/debenture_list.naver?page=170 (Caused by NameResolutionError(\"HTTPSConnection(host='finance.naver.com', port=443): Failed to resolve 'finance.naver.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[Page 171] 에러 발생: HTTPSConnectionPool(host='finance.naver.com', port=443): Max retries exceeded with url: /research/debenture_list.naver?page=171 (Caused by NameResolutionError(\"HTTPSConnection(host='finance.naver.com', port=443): Failed to resolve 'finance.naver.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[Page 172] 30건 수집 완료 (누적: 4980건)\n",
      "[Page 173] 30건 수집 완료 (누적: 5010건)\n",
      "[Page 174] 30건 수집 완료 (누적: 5040건)\n",
      "[Page 175] 30건 수집 완료 (누적: 5070건)\n",
      "[Page 176] 30건 수집 완료 (누적: 5100건)\n",
      "[Page 177] 30건 수집 완료 (누적: 5130건)\n",
      "[Page 178] 30건 수집 완료 (누적: 5160건)\n",
      "[Page 179] 30건 수집 완료 (누적: 5190건)\n",
      "[Page 180] 30건 수집 완료 (누적: 5220건)\n",
      "[Page 181] 30건 수집 완료 (누적: 5250건)\n",
      "[Page 182] 30건 수집 완료 (누적: 5280건)\n",
      "[Page 183] 30건 수집 완료 (누적: 5310건)\n",
      "[Page 184] 30건 수집 완료 (누적: 5340건)\n",
      "[Page 185] 30건 수집 완료 (누적: 5370건)\n",
      "[Page 186] 30건 수집 완료 (누적: 5400건)\n",
      "[Page 187] 30건 수집 완료 (누적: 5430건)\n",
      "[Page 188] 30건 수집 완료 (누적: 5460건)\n",
      "[Page 189] 30건 수집 완료 (누적: 5490건)\n",
      "[Page 190] 30건 수집 완료 (누적: 5520건)\n",
      "[Page 191] 30건 수집 완료 (누적: 5550건)\n",
      "[Page 192] 30건 수집 완료 (누적: 5580건)\n",
      "[Page 193] 30건 수집 완료 (누적: 5610건)\n",
      "[Page 194] 30건 수집 완료 (누적: 5640건)\n",
      "[Page 195] 30건 수집 완료 (누적: 5670건)\n",
      "[Page 196] 30건 수집 완료 (누적: 5700건)\n",
      "[Page 197] 30건 수집 완료 (누적: 5730건)\n",
      "[Page 198] 30건 수집 완료 (누적: 5760건)\n",
      "[Page 199] 30건 수집 완료 (누적: 5790건)\n",
      "[Page 200] 30건 수집 완료 (누적: 5820건)\n",
      "[Page 201] 30건 수집 완료 (누적: 5850건)\n",
      "[Page 202] 30건 수집 완료 (누적: 5880건)\n",
      "[Page 203] 30건 수집 완료 (누적: 5910건)\n",
      "[Page 204] 30건 수집 완료 (누적: 5940건)\n",
      "[Page 205] 30건 수집 완료 (누적: 5970건)\n",
      "[Page 206] 30건 수집 완료 (누적: 6000건)\n",
      "[Page 207] 30건 수집 완료 (누적: 6030건)\n",
      "[Page 208] 30건 수집 완료 (누적: 6060건)\n",
      "[Page 209] 30건 수집 완료 (누적: 6090건)\n",
      "[Page 210] 30건 수집 완료 (누적: 6120건)\n",
      "[Page 211] 30건 수집 완료 (누적: 6150건)\n",
      "[Page 212] 30건 수집 완료 (누적: 6180건)\n",
      "[Page 213] 30건 수집 완료 (누적: 6210건)\n",
      "[Page 214] 30건 수집 완료 (누적: 6240건)\n",
      "[Page 215] 30건 수집 완료 (누적: 6270건)\n",
      "[Page 216] 30건 수집 완료 (누적: 6300건)\n",
      "[Page 217] 30건 수집 완료 (누적: 6330건)\n",
      "[Page 218] 30건 수집 완료 (누적: 6360건)\n",
      "[Page 219] 30건 수집 완료 (누적: 6390건)\n",
      "[Page 220] 30건 수집 완료 (누적: 6420건)\n",
      "[Page 221] 30건 수집 완료 (누적: 6450건)\n",
      "[Page 222] 30건 수집 완료 (누적: 6480건)\n",
      "[Page 223] 30건 수집 완료 (누적: 6510건)\n",
      "[Page 224] 30건 수집 완료 (누적: 6540건)\n",
      "[Page 225] 30건 수집 완료 (누적: 6570건)\n",
      "[Page 226] 30건 수집 완료 (누적: 6600건)\n",
      "[Page 227] 30건 수집 완료 (누적: 6630건)\n",
      "[Page 228] 30건 수집 완료 (누적: 6660건)\n",
      "[Page 229] 30건 수집 완료 (누적: 6690건)\n",
      "[Page 230] 30건 수집 완료 (누적: 6720건)\n",
      "[Page 231] 30건 수집 완료 (누적: 6750건)\n",
      "[Page 232] 30건 수집 완료 (누적: 6780건)\n",
      "[Page 233] 30건 수집 완료 (누적: 6810건)\n",
      "[Page 234] 30건 수집 완료 (누적: 6840건)\n",
      "[Page 235] 30건 수집 완료 (누적: 6870건)\n",
      "[Page 236] 30건 수집 완료 (누적: 6900건)\n",
      "[Page 237] 30건 수집 완료 (누적: 6930건)\n",
      "[Page 238] 30건 수집 완료 (누적: 6960건)\n",
      "[Page 239] 30건 수집 완료 (누적: 6990건)\n",
      "[Page 240] 30건 수집 완료 (누적: 7020건)\n",
      "[Page 241] 30건 수집 완료 (누적: 7050건)\n",
      "[Page 242] 30건 수집 완료 (누적: 7080건)\n",
      "[Page 243] 30건 수집 완료 (누적: 7110건)\n",
      "[Page 244] 30건 수집 완료 (누적: 7140건)\n",
      "[Page 245] 30건 수집 완료 (누적: 7170건)\n",
      "[Page 246] 30건 수집 완료 (누적: 7200건)\n",
      "[Page 247] 30건 수집 완료 (누적: 7230건)\n",
      "[Page 248] 30건 수집 완료 (누적: 7260건)\n",
      "[Page 249] 30건 수집 완료 (누적: 7290건)\n",
      "[Page 250] 30건 수집 완료 (누적: 7320건)\n",
      "[Page 251] 30건 수집 완료 (누적: 7350건)\n",
      "[Page 252] 30건 수집 완료 (누적: 7380건)\n",
      "[Page 253] 30건 수집 완료 (누적: 7410건)\n",
      "[Page 254] 30건 수집 완료 (누적: 7440건)\n",
      "[Page 255] 30건 수집 완료 (누적: 7470건)\n",
      "[Page 256] 30건 수집 완료 (누적: 7500건)\n",
      "[Page 257] 30건 수집 완료 (누적: 7530건)\n",
      "[Page 258] 30건 수집 완료 (누적: 7560건)\n",
      "[Page 259] 30건 수집 완료 (누적: 7590건)\n",
      "[Page 260] 30건 수집 완료 (누적: 7620건)\n",
      "[Page 261] 30건 수집 완료 (누적: 7650건)\n",
      "[Page 262] 30건 수집 완료 (누적: 7680건)\n",
      "[Page 263] 30건 수집 완료 (누적: 7710건)\n",
      "[Page 264] 30건 수집 완료 (누적: 7740건)\n",
      "[Page 265] 30건 수집 완료 (누적: 7770건)\n",
      "[Page 266] 30건 수집 완료 (누적: 7800건)\n",
      "[Page 267] 30건 수집 완료 (누적: 7830건)\n",
      "[Page 268] 30건 수집 완료 (누적: 7860건)\n",
      "[Page 269] 30건 수집 완료 (누적: 7890건)\n",
      "[Page 270] 30건 수집 완료 (누적: 7920건)\n",
      "[Page 271] 30건 수집 완료 (누적: 7950건)\n",
      "[Page 272] 30건 수집 완료 (누적: 7980건)\n",
      "[Page 273] 30건 수집 완료 (누적: 8010건)\n",
      "[Page 274] 30건 수집 완료 (누적: 8040건)\n",
      "[Page 275] 30건 수집 완료 (누적: 8070건)\n",
      "[Page 276] 30건 수집 완료 (누적: 8100건)\n",
      "[Page 277] 30건 수집 완료 (누적: 8130건)\n",
      "[Page 278] 30건 수집 완료 (누적: 8160건)\n",
      "[Page 279] 30건 수집 완료 (누적: 8190건)\n",
      "[Page 280] 30건 수집 완료 (누적: 8220건)\n",
      "[Page 281] 30건 수집 완료 (누적: 8250건)\n",
      "[Page 282] 30건 수집 완료 (누적: 8280건)\n",
      "[Page 283] 30건 수집 완료 (누적: 8310건)\n",
      "[Page 284] 30건 수집 완료 (누적: 8340건)\n",
      "[Page 285] 30건 수집 완료 (누적: 8370건)\n",
      "[Page 286] 30건 수집 완료 (누적: 8400건)\n",
      "[Page 287] 30건 수집 완료 (누적: 8430건)\n",
      "[Page 288] 30건 수집 완료 (누적: 8460건)\n",
      "[Page 289] 30건 수집 완료 (누적: 8490건)\n",
      "[Page 290] 30건 수집 완료 (누적: 8520건)\n",
      "[Page 291] 30건 수집 완료 (누적: 8550건)\n",
      "[Page 292] 30건 수집 완료 (누적: 8580건)\n",
      "[Page 293] 30건 수집 완료 (누적: 8610건)\n",
      "[Page 294] 30건 수집 완료 (누적: 8640건)\n",
      "[Page 295] 30건 수집 완료 (누적: 8670건)\n",
      "[Page 296] 30건 수집 완료 (누적: 8700건)\n",
      "[Page 297] 30건 수집 완료 (누적: 8730건)\n",
      "[Page 298] 30건 수집 완료 (누적: 8760건)\n",
      "[Page 299] 30건 수집 완료 (누적: 8790건)\n",
      "[Page 300] 30건 수집 완료 (누적: 8820건)\n",
      "[Page 301] 30건 수집 완료 (누적: 8850건)\n",
      "[Page 302] 30건 수집 완료 (누적: 8880건)\n",
      "[Page 303] 30건 수집 완료 (누적: 8910건)\n",
      "[Page 304] 30건 수집 완료 (누적: 8940건)\n",
      "[Page 305] 30건 수집 완료 (누적: 8970건)\n",
      "[Page 306] 30건 수집 완료 (누적: 9000건)\n",
      "[Page 307] 30건 수집 완료 (누적: 9030건)\n",
      "[Page 308] 30건 수집 완료 (누적: 9060건)\n",
      "[Page 309] 30건 수집 완료 (누적: 9090건)\n",
      "[Page 310] 30건 수집 완료 (누적: 9120건)\n",
      "[Page 311] 30건 수집 완료 (누적: 9150건)\n",
      "[Page 312] 30건 수집 완료 (누적: 9180건)\n",
      "[Page 313] 30건 수집 완료 (누적: 9210건)\n",
      "[Page 314] 30건 수집 완료 (누적: 9240건)\n",
      "[Page 315] 30건 수집 완료 (누적: 9270건)\n",
      "[Page 316] 30건 수집 완료 (누적: 9300건)\n",
      "[Page 317] 30건 수집 완료 (누적: 9330건)\n",
      "[Page 318] 30건 수집 완료 (누적: 9360건)\n",
      "[Page 319] 30건 수집 완료 (누적: 9390건)\n",
      "[Page 320] 30건 수집 완료 (누적: 9420건)\n",
      "[Page 321] 9건 수집 완료 (누적: 9429건)\n",
      "[Page 322] 9건 수집 완료 (누적: 9438건)\n",
      "[Page 323] 9건 수집 완료 (누적: 9447건)\n",
      "[Page 324] 9건 수집 완료 (누적: 9456건)\n",
      "[Page 325] 9건 수집 완료 (누적: 9465건)\n",
      "[Page 326] 9건 수집 완료 (누적: 9474건)\n",
      "[Page 327] 9건 수집 완료 (누적: 9483건)\n",
      "[Page 328] 9건 수집 완료 (누적: 9492건)\n",
      "[Page 329] 9건 수집 완료 (누적: 9501건)\n",
      "[Page 330] 9건 수집 완료 (누적: 9510건)\n",
      "[Page 331] 9건 수집 완료 (누적: 9519건)\n",
      "[Page 332] 9건 수집 완료 (누적: 9528건)\n",
      "[Page 333] 9건 수집 완료 (누적: 9537건)\n",
      "[Page 334] 9건 수집 완료 (누적: 9546건)\n",
      "[Page 335] 9건 수집 완료 (누적: 9555건)\n",
      "[Page 336] 9건 수집 완료 (누적: 9564건)\n",
      "[Page 337] 9건 수집 완료 (누적: 9573건)\n",
      "[Page 338] 9건 수집 완료 (누적: 9582건)\n",
      "[Page 339] 9건 수집 완료 (누적: 9591건)\n",
      "[Page 340] 9건 수집 완료 (누적: 9600건)\n",
      "[Page 341] 9건 수집 완료 (누적: 9609건)\n",
      "[Page 342] 9건 수집 완료 (누적: 9618건)\n",
      "[Page 343] 9건 수집 완료 (누적: 9627건)\n",
      "[Page 344] 9건 수집 완료 (누적: 9636건)\n",
      "[Page 345] 9건 수집 완료 (누적: 9645건)\n",
      "[Page 346] 9건 수집 완료 (누적: 9654건)\n",
      "[Page 347] 9건 수집 완료 (누적: 9663건)\n",
      "[Page 348] 9건 수집 완료 (누적: 9672건)\n",
      "[Page 349] 9건 수집 완료 (누적: 9681건)\n",
      "[Page 350] 9건 수집 완료 (누적: 9690건)\n",
      "[Page 351] 9건 수집 완료 (누적: 9699건)\n",
      "[Page 352] 9건 수집 완료 (누적: 9708건)\n",
      "[Page 353] 9건 수집 완료 (누적: 9717건)\n",
      "[Page 354] 9건 수집 완료 (누적: 9726건)\n",
      "[Page 355] 9건 수집 완료 (누적: 9735건)\n",
      "[Page 356] 9건 수집 완료 (누적: 9744건)\n",
      "[Page 357] 9건 수집 완료 (누적: 9753건)\n",
      "[Page 358] 9건 수집 완료 (누적: 9762건)\n",
      "[Page 359] 9건 수집 완료 (누적: 9771건)\n",
      "[Page 360] 9건 수집 완료 (누적: 9780건)\n",
      "[Page 361] 9건 수집 완료 (누적: 9789건)\n",
      "[Page 362] 9건 수집 완료 (누적: 9798건)\n",
      "[Page 363] 9건 수집 완료 (누적: 9807건)\n",
      "[Page 364] 9건 수집 완료 (누적: 9816건)\n",
      "[Page 365] 9건 수집 완료 (누적: 9825건)\n",
      "[Page 366] 9건 수집 완료 (누적: 9834건)\n",
      "[Page 367] 9건 수집 완료 (누적: 9843건)\n",
      "[Page 368] 9건 수집 완료 (누적: 9852건)\n",
      "[Page 369] 9건 수집 완료 (누적: 9861건)\n",
      "[Page 370] 9건 수집 완료 (누적: 9870건)\n",
      "[Page 371] 9건 수집 완료 (누적: 9879건)\n",
      "[Page 372] 9건 수집 완료 (누적: 9888건)\n",
      "[Page 373] 9건 수집 완료 (누적: 9897건)\n",
      "[Page 374] 9건 수집 완료 (누적: 9906건)\n",
      "[Page 375] 9건 수집 완료 (누적: 9915건)\n",
      "[Page 376] 9건 수집 완료 (누적: 9924건)\n",
      "[Page 377] 9건 수집 완료 (누적: 9933건)\n",
      "[Page 378] 9건 수집 완료 (누적: 9942건)\n",
      "[Page 379] 9건 수집 완료 (누적: 9951건)\n",
      "[Page 380] 9건 수집 완료 (누적: 9960건)\n",
      "[Page 381] 9건 수집 완료 (누적: 9969건)\n",
      "[Page 382] 9건 수집 완료 (누적: 9978건)\n",
      "[Page 383] 9건 수집 완료 (누적: 9987건)\n",
      "[Page 384] 9건 수집 완료 (누적: 9996건)\n",
      "[Page 385] 9건 수집 완료 (누적: 10005건)\n",
      "[Page 386] 9건 수집 완료 (누적: 10014건)\n",
      "[Page 387] 9건 수집 완료 (누적: 10023건)\n",
      "[Page 388] 9건 수집 완료 (누적: 10032건)\n",
      "[Page 389] 9건 수집 완료 (누적: 10041건)\n",
      "[Page 390] 9건 수집 완료 (누적: 10050건)\n",
      "[Page 391] 9건 수집 완료 (누적: 10059건)\n",
      "[Page 392] 9건 수집 완료 (누적: 10068건)\n",
      "[Page 393] 9건 수집 완료 (누적: 10077건)\n",
      "[Page 394] 9건 수집 완료 (누적: 10086건)\n",
      "[Page 395] 9건 수집 완료 (누적: 10095건)\n",
      "[Page 396] 9건 수집 완료 (누적: 10104건)\n",
      "[Page 397] 9건 수집 완료 (누적: 10113건)\n",
      "[Page 398] 9건 수집 완료 (누적: 10122건)\n",
      "[Page 399] 9건 수집 완료 (누적: 10131건)\n",
      "[Page 400] 9건 수집 완료 (누적: 10140건)\n",
      "[Page 401] 9건 수집 완료 (누적: 10149건)\n",
      "[Page 402] 9건 수집 완료 (누적: 10158건)\n",
      "[Page 403] 9건 수집 완료 (누적: 10167건)\n",
      "[Page 404] 9건 수집 완료 (누적: 10176건)\n",
      "[Page 405] 9건 수집 완료 (누적: 10185건)\n",
      "[Page 406] 9건 수집 완료 (누적: 10194건)\n",
      "[Page 407] 9건 수집 완료 (누적: 10203건)\n",
      "[Page 408] 9건 수집 완료 (누적: 10212건)\n",
      "[Page 409] 9건 수집 완료 (누적: 10221건)\n",
      "[Page 410] 9건 수집 완료 (누적: 10230건)\n",
      "[Page 411] 9건 수집 완료 (누적: 10239건)\n",
      "[Page 412] 9건 수집 완료 (누적: 10248건)\n",
      "[Page 413] 9건 수집 완료 (누적: 10257건)\n",
      "[Page 414] 9건 수집 완료 (누적: 10266건)\n",
      "[Page 415] 9건 수집 완료 (누적: 10275건)\n",
      "[Page 416] 9건 수집 완료 (누적: 10284건)\n",
      "[Page 417] 9건 수집 완료 (누적: 10293건)\n",
      "[Page 418] 9건 수집 완료 (누적: 10302건)\n",
      "[Page 419] 9건 수집 완료 (누적: 10311건)\n",
      "[Page 420] 9건 수집 완료 (누적: 10320건)\n",
      "[Page 421] 9건 수집 완료 (누적: 10329건)\n",
      "[Page 422] 9건 수집 완료 (누적: 10338건)\n",
      "[Page 423] 9건 수집 완료 (누적: 10347건)\n",
      "[Page 424] 9건 수집 완료 (누적: 10356건)\n",
      "[Page 425] 9건 수집 완료 (누적: 10365건)\n",
      "[Page 426] 9건 수집 완료 (누적: 10374건)\n",
      "[Page 427] 9건 수집 완료 (누적: 10383건)\n",
      "[Page 428] 9건 수집 완료 (누적: 10392건)\n",
      "[Page 429] 9건 수집 완료 (누적: 10401건)\n",
      "[Page 430] 9건 수집 완료 (누적: 10410건)\n",
      "[Page 431] 9건 수집 완료 (누적: 10419건)\n",
      "[Page 432] 9건 수집 완료 (누적: 10428건)\n",
      "[Page 433] 9건 수집 완료 (누적: 10437건)\n",
      "[Page 434] 9건 수집 완료 (누적: 10446건)\n",
      "[Page 435] 9건 수집 완료 (누적: 10455건)\n",
      "[Page 436] 9건 수집 완료 (누적: 10464건)\n",
      "[Page 437] 9건 수집 완료 (누적: 10473건)\n",
      "[Page 438] 9건 수집 완료 (누적: 10482건)\n",
      "[Page 439] 9건 수집 완료 (누적: 10491건)\n",
      "[Page 440] 9건 수집 완료 (누적: 10500건)\n",
      "[Page 441] 9건 수집 완료 (누적: 10509건)\n",
      "[Page 442] 9건 수집 완료 (누적: 10518건)\n",
      "[Page 443] 9건 수집 완료 (누적: 10527건)\n",
      "[Page 444] 9건 수집 완료 (누적: 10536건)\n",
      "[Page 445] 9건 수집 완료 (누적: 10545건)\n",
      "[Page 446] 9건 수집 완료 (누적: 10554건)\n",
      "[Page 447] 9건 수집 완료 (누적: 10563건)\n",
      "[Page 448] 9건 수집 완료 (누적: 10572건)\n",
      "[Page 449] 9건 수집 완료 (누적: 10581건)\n",
      "[Page 450] 9건 수집 완료 (누적: 10590건)\n",
      "[Page 451] 9건 수집 완료 (누적: 10599건)\n",
      "[Page 452] 9건 수집 완료 (누적: 10608건)\n",
      "[Page 453] 9건 수집 완료 (누적: 10617건)\n",
      "[Page 454] 9건 수집 완료 (누적: 10626건)\n",
      "[Page 455] 9건 수집 완료 (누적: 10635건)\n",
      "[Page 456] 9건 수집 완료 (누적: 10644건)\n",
      "[Page 457] 9건 수집 완료 (누적: 10653건)\n",
      "[Page 458] 9건 수집 완료 (누적: 10662건)\n",
      "[Page 459] 9건 수집 완료 (누적: 10671건)\n",
      "[Page 460] 9건 수집 완료 (누적: 10680건)\n",
      "[Page 461] 9건 수집 완료 (누적: 10689건)\n",
      "[Page 462] 9건 수집 완료 (누적: 10698건)\n",
      "[Page 463] 9건 수집 완료 (누적: 10707건)\n",
      "[Page 464] 9건 수집 완료 (누적: 10716건)\n",
      "[Page 465] 9건 수집 완료 (누적: 10725건)\n",
      "[Page 466] 9건 수집 완료 (누적: 10734건)\n",
      "[Page 467] 9건 수집 완료 (누적: 10743건)\n",
      "[Page 468] 9건 수집 완료 (누적: 10752건)\n",
      "[Page 469] 9건 수집 완료 (누적: 10761건)\n",
      "[Page 470] 9건 수집 완료 (누적: 10770건)\n",
      "[Page 471] 9건 수집 완료 (누적: 10779건)\n",
      "[Page 472] 9건 수집 완료 (누적: 10788건)\n",
      "[Page 473] 9건 수집 완료 (누적: 10797건)\n",
      "[Page 474] 9건 수집 완료 (누적: 10806건)\n",
      "[Page 475] 9건 수집 완료 (누적: 10815건)\n",
      "[Page 476] 9건 수집 완료 (누적: 10824건)\n",
      "[Page 477] 9건 수집 완료 (누적: 10833건)\n",
      "[Page 478] 9건 수집 완료 (누적: 10842건)\n",
      "[Page 479] 9건 수집 완료 (누적: 10851건)\n",
      "[Page 480] 9건 수집 완료 (누적: 10860건)\n",
      "[Page 481] 9건 수집 완료 (누적: 10869건)\n",
      "[Page 482] 9건 수집 완료 (누적: 10878건)\n",
      "[Page 483] 9건 수집 완료 (누적: 10887건)\n",
      "[Page 484] 9건 수집 완료 (누적: 10896건)\n",
      "[Page 485] 9건 수집 완료 (누적: 10905건)\n",
      "[Page 486] 9건 수집 완료 (누적: 10914건)\n",
      "[Page 487] 9건 수집 완료 (누적: 10923건)\n",
      "[Page 488] 9건 수집 완료 (누적: 10932건)\n",
      "[Page 489] 9건 수집 완료 (누적: 10941건)\n",
      "[Page 490] 9건 수집 완료 (누적: 10950건)\n",
      "[Page 491] 9건 수집 완료 (누적: 10959건)\n",
      "[Page 492] 9건 수집 완료 (누적: 10968건)\n",
      "[Page 493] 9건 수집 완료 (누적: 10977건)\n",
      "[Page 494] 9건 수집 완료 (누적: 10986건)\n",
      "[Page 495] 9건 수집 완료 (누적: 10995건)\n",
      "[Page 496] 9건 수집 완료 (누적: 11004건)\n",
      "[Page 497] 9건 수집 완료 (누적: 11013건)\n",
      "[Page 498] 9건 수집 완료 (누적: 11022건)\n",
      "[Page 499] 9건 수집 완료 (누적: 11031건)\n",
      "[Page 500] 9건 수집 완료 (누적: 11040건)\n",
      "전체 리스트가 naver_bond_all_list.csv로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 수집할 총 리포트 목록을 담을 리스트\n",
    "all_bond_data = []\n",
    "\n",
    "# 시작 페이지와 끝 페이지 설정 (2005년까지 가려면 수백 페이지가 필요할 수 있습니다)\n",
    "# 우선 1페이지부터 100페이지까지 시도해보고, 더 필요하면 숫자를 늘리세요.\n",
    "start_page = 1\n",
    "end_page = 500 # 네이버 채권 끝 페이지까지 확인 필요\n",
    "\n",
    "print(f\"{start_page}페이지부터 수집을 시작합니다. 대장정의 시작!\")\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    page_url = f\"https://finance.naver.com/research/debenture_list.naver?page={page}\"\n",
    "    \n",
    "    try:\n",
    "        # 1. 페이지 요청\n",
    "        res = session.get(page_url, headers=headers)\n",
    "        res.encoding = 'euc-kr'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # 2. 리스트 추출 (강사님이 짚어준 type_1)\n",
    "        rows = soup.select('table.type_1 tr')\n",
    "        \n",
    "        # 이번 페이지에서 새로 찾은 데이터 개수 확인\n",
    "        current_page_count = 0\n",
    "        for row in rows:\n",
    "            tds = row.find_all('td')\n",
    "            if len(tds) >= 4:\n",
    "                title_tag = tds[0].find('a')\n",
    "                if title_tag:\n",
    "                    title = title_tag.text.strip()\n",
    "                    company = tds[1].text.strip()\n",
    "                    date = tds[4].text.strip()\n",
    "                    link = \"https://finance.naver.com/research/\" + title_tag['href']\n",
    "                    \n",
    "                    all_bond_data.append([date, company, title, link])\n",
    "                    current_page_count += 1\n",
    "        \n",
    "        print(f\"[Page {page}] {current_page_count}건 수집 완료 (누적: {len(all_bond_data)}건)\")\n",
    "        \n",
    "        # 3. 브레이크 포인트 (데이터가 없는 페이지에 도달하면 멈춤)\n",
    "        if current_page_count == 0:\n",
    "            print(\"더 이상 데이터가 없는 페이지입니다. 수집을 종료합니다.\")\n",
    "            break\n",
    "            \n",
    "        # 4. 서버 과부하 방지 (중요!)\n",
    "        # 너무 빨리 요청하면 IP 차단당하니 0.3초 정도 쉬어줍니다.\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Page {page}] 에러 발생: {e}\")\n",
    "        continue\n",
    "\n",
    "# 전체 목록 저장\n",
    "final_df = pd.DataFrame(all_bond_data, columns=['날짜', '증권사', '제목', '링크'])\n",
    "final_df.to_csv(\"naver_bond_all_list.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"전체 리스트가 naver_bond_all_list.csv로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4058fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pdfplumber\n",
    "import io\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# [1] 지저분한 경고 메시지(WARNING) 출력 차단\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "# [2] 저장 경로 설정 (v5로 새로 시작)\n",
    "save_dir = '/content/drive/MyDrive/bond_text_final_v5'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "session = requests.Session()\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0'}\n",
    "\n",
    "page = 1\n",
    "target_date = \"05.05.01\"\n",
    "stop_collecting = False\n",
    "\n",
    "print(\"🚀 수집 시작! 이제 날짜가 정확한 것만 화면에 보입니다.\")\n",
    "\n",
    "while not stop_collecting and page < 2000:\n",
    "    url = f\"https://finance.naver.com/research/debenture_list.naver?page={page}\"\n",
    "    try:\n",
    "        res = session.get(url, headers=headers)\n",
    "        res.encoding = 'euc-kr'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # [3] 날짜가 포함된 td를 더 정확하게 타겟팅 (class가 'date'인 것 찾기)\n",
    "        rows = soup.select('table.type_1 tr')\n",
    "        \n",
    "        found_on_page = 0\n",
    "        for row in rows:\n",
    "            # 날짜 칸(보통 마지막 td이거나 class=\"date\"인 td)을 찾습니다.\n",
    "            date_td = row.select_one('td.date')\n",
    "            if not date_td:\n",
    "                # class가 없다면 모든 td 중 날짜 형식인 것을 찾음\n",
    "                tds = row.find_all('td')\n",
    "                if not tds: continue\n",
    "                raw_date = tds[-1].text.strip()\n",
    "            else:\n",
    "                raw_date = date_td.text.strip()\n",
    "\n",
    "            # [4] 정규식으로 '24.12.30' 같은 형식인지 엄격하게 검사\n",
    "            if not re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                continue # 20844 같은 숫자는 여기서 무조건 걸러집니다.\n",
    "\n",
    "            if raw_date < target_date:\n",
    "                stop_collecting = True\n",
    "                break\n",
    "            \n",
    "            # 날짜 변환 (예: 20251230)\n",
    "            clean_date = \"20\" + raw_date.replace(\".\", \"\")\n",
    "            \n",
    "            # 증권사 및 제목 추출\n",
    "            tds = row.find_all('td')\n",
    "            company = tds[1].text.strip().replace(\" \", \"\")\n",
    "            title_tag = tds[0].find('a')\n",
    "            if not title_tag: continue\n",
    "            \n",
    "            title = title_tag.text.strip()\n",
    "            detail_url = \"https://finance.naver.com/research/\" + title_tag['href']\n",
    "            \n",
    "            # 파일명 생성 (공백 없이 날짜가 맨 앞으로!)\n",
    "            safe_title = re.sub(r'[^\\uAC00-\\uD7A30-9a-zA-Z]', '', title)\n",
    "            file_name = f\"{clean_date}_{company}_{safe_title[:20]}.txt\"\n",
    "            file_path = os.path.join(save_dir, file_name)\n",
    "            \n",
    "            if os.path.exists(file_path): continue\n",
    "\n",
    "            try:\n",
    "                # PDF 추출\n",
    "                d_res = session.get(detail_url, headers=headers)\n",
    "                d_res.encoding = 'euc-kr'\n",
    "                pdf_url = None\n",
    "                d_soup = BeautifulSoup(d_res.text, 'html.parser')\n",
    "                for a in d_soup.find_all('a', href=True):\n",
    "                    if '.pdf' in a['href'].lower():\n",
    "                        pdf_url = a['href']\n",
    "                        break\n",
    "                \n",
    "                if pdf_url:\n",
    "                    p_res = session.get(pdf_url, headers=headers)\n",
    "                    with pdfplumber.open(io.BytesIO(p_res.content)) as pdf:\n",
    "                        full_text = \"\\n\".join([p.extract_text() for p in pdf.pages if p.extract_text()])\n",
    "                    \n",
    "                    if full_text.strip():\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(full_text)\n",
    "                        print(f\"✅ [저장성공] {file_name}\") # 이 메시지만 보일 겁니다!\n",
    "                        found_on_page += 1\n",
    "                \n",
    "                time.sleep(random.uniform(0.3, 0.5))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if found_on_page > 0:\n",
    "            print(f\"--- {page}페이지 작업 완료 ---\")\n",
    "        \n",
    "        page += 1\n",
    "        # 네이버가 1000페이지 넘어가면 데이터를 안 줄 때를 대비\n",
    "        if found_on_page == 0 and page > 100:\n",
    "            # 5페이지 연속으로 데이터를 못 찾으면 진짜 끝난 것으로 간주\n",
    "            empty_check_count = vars().get('empty_check_count', 0) + 1\n",
    "            if empty_check_count > 5: break\n",
    "        else:\n",
    "            empty_check_count = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🌐 접속 에러: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"🎉 수집이 완료되었습니다! 드라이브 v5 폴더를 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
