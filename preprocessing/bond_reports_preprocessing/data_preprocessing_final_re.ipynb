{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3a4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fce9d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekonlpy in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (3.9.2)\n",
      "Requirement already satisfied: scipy<=1.13.1,>1.10.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (1.13.1)\n",
      "Requirement already satisfied: pandas<=2.3.2,>=1.5.3 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (2.3.2)\n",
      "Requirement already satisfied: click>=8.1.6 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (8.1.8)\n",
      "Requirement already satisfied: fugashi>=1.3.3 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (1.5.2)\n",
      "Requirement already satisfied: mecab-ko-dic>=1.0.0 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ekonlpy) (1.0.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (1.5.3)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scipy<=1.13.1,>1.10.0->ekonlpy) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.3)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from click>=8.1.6->ekonlpy) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas<=2.3.2,>=1.5.3->ekonlpy) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install ekonlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c3b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” íŒŒì¼ í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "âœ… í•„í„°ë§ ì™„ë£Œ! 7722ê°œì˜ íŒŒì¼ì´ 'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\bond_text_2012_to_2025'ë¡œ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "##ë°ì´í„° í•„í„°ë§(2012ë…„ ì´í›„ íŒŒì¼ë§Œ ì¬ìˆ˜ì§‘)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. ì›ë³¸ íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ (ë³¸ì¸ì˜ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "source_dir = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\bond_text_final_v5'\n",
    "\n",
    "# 2. 2012ë…„ ì´í›„ íŒŒì¼ë§Œ ë”°ë¡œ ëª¨ì„ ìƒˆ í´ë” ê²½ë¡œ\n",
    "target_dir = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\bond_text_2012_to_2025'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# 3. ê¸°ì¤€ ë‚ ì§œ ì„¤ì •\n",
    "start_date = \"20120101\"\n",
    "\n",
    "print(\"ğŸ” íŒŒì¼ í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(source_dir):\n",
    "    # íŒŒì¼ëª… ì• 8ìë¦¬ê°€ ë‚ ì§œë¼ê³  ê°€ì • (ì˜ˆ: 20120115_ì¦ê¶Œì‚¬_ì œëª©.txt)\n",
    "    file_date = filename[:8]\n",
    "    \n",
    "    # ìˆ«ìë¡œ ëœ ë‚ ì§œì¸ì§€ í™•ì¸í•˜ê³  ê¸°ì¤€ì¼ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©´ ë³µì‚¬\n",
    "    if file_date.isdigit() and file_date >= start_date:\n",
    "        src_path = os.path.join(source_dir, filename)\n",
    "        dst_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        shutil.copy2(src_path, dst_path) # íŒŒì¼ ë³µì‚¬ (ì›ë³¸ ìœ ì§€)\n",
    "        count += 1\n",
    "\n",
    "print(f\"âœ… í•„í„°ë§ ì™„ë£Œ! {count}ê°œì˜ íŒŒì¼ì´ '{target_dir}'ë¡œ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c38501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\eeyy1\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\eeyy1\\appdata\\roaming\\python\\python39\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7c65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ í´ë” ë‚´ íŒŒì¼ ì½ê¸° ì‹œì‘: C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\bond_text_2012_to_2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í†µí•© ì¤‘:   0%|          | 0/7722 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í†µí•© ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7722/7722 [00:09<00:00, 793.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "âœ… ì‘ì—… ì™„ë£Œ! ì†Œìˆ˜ì ì´ ë³´í˜¸ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“Š ì´ í†µí•© ë‚ ì§œ ìˆ˜: 2466ì¼\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "source_dir = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\bond_text_2012_to_2025'\n",
    "save_path = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\integrated_reports_2012_2025.csv'\n",
    "\n",
    "# --- [ìˆ˜ì •ëœ í•¨ìˆ˜ 1] í…ìŠ¤íŠ¸ ì •ì œ (ìˆ«ìì™€ ë§ˆì¹¨í‘œ ë³´ì¡´) ---\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. ì €ì‘ê¶Œ ë° ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±° (ì´ì „ê³¼ ë™ì¼)\n",
    "    boilerplate_patterns = [\n",
    "        r\"ë³¸ ìë£Œì— ìˆ˜ë¡ëœ.*\", r\"ë³¸ ìë£ŒëŠ” ì–´ë– í•œ.*\", r\"Copyright.*\", \n",
    "        r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", \n",
    "        r\"https?://\\S+|www\\.\\S+\", r\"Tel:.*\",\n",
    "    ]\n",
    "    for pattern in boilerplate_patterns:\n",
    "        text = re.sub(pattern, \" \", text)\n",
    "\n",
    "    # 2. [ì¤‘ìš”] ì¤„ë°”ê¿ˆ(\\n)ì´ë‚˜ íƒ­(\\t)ì„ ë§ˆì¹¨í‘œì™€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜\n",
    "    # ë§ˆì¹¨í‘œ ì—†ì´ ì¤„ë°”ê¿ˆë§Œ ëœ ë¬¸ì¥ë“¤ì„ ê°•ì œë¡œ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.\n",
    "    text = text.replace('\\n', '. ').replace('\\t', ' ')\n",
    "\n",
    "    # 3. í•œê¸€, ìˆ«ì, ê³µë°±, ë§ˆì¹¨í‘œ(.)ë§Œ ë‚¨ê¸°ê¸°\n",
    "    text = re.sub(r'[^ê°€-í£0-9\\s\\.]', ' ', text)\n",
    "    \n",
    "    # 4. ì—°ì†ëœ ë§ˆì¹¨í‘œ(.. )ë‚˜ ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\.+', '.', text) # .. -> .\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# --- [ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜ 2] ì†Œìˆ˜ì  ë³´í˜¸ ë¬¸ì¥ ë¶„ë¦¬ ---\n",
    "def split_sentences(text):\n",
    "    # ì†Œìˆ˜ì (3.25)ì€ ë¬´ì‹œí•˜ê³ , í•œê¸€ ë’¤ì— ì˜¤ëŠ” ë§ˆì¹¨í‘œë§Œ ë¬¸ì¥ì˜ ëìœ¼ë¡œ ì¸ì‹\n",
    "    # ì´ ë¡œì§ì´ ìˆì–´ì•¼ ë‚˜ì¤‘ì— ë¬¸ì¥ë³„ ì ìˆ˜ë¥¼ ë§¤ê¸¸ ë•Œ ì •í™•í•´ì§‘ë‹ˆë‹¤.\n",
    "    sentence_split_pattern = r'(?<=[ê°€-í£])\\.(?=\\s|[ê°€-í£]|$)'\n",
    "    sentences = re.split(sentence_split_pattern, text)\n",
    "    return [sent.strip() + \".\" for sent in sentences if len(sent.strip()) > 5]\n",
    "\n",
    "# 2. ë‚ ì§œë³„ë¡œ í…ìŠ¤íŠ¸ í†µí•©í•˜ê¸°\n",
    "data_dict = {}\n",
    "\n",
    "print(f\"ğŸ“‚ í´ë” ë‚´ íŒŒì¼ ì½ê¸° ì‹œì‘: {source_dir}\")\n",
    "filenames = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n",
    "\n",
    "for filename in tqdm(filenames, desc=\"í†µí•© ì¤‘\"):\n",
    "    file_date = filename[:8]\n",
    "    file_path = os.path.join(source_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        cleaned_content = clean_text(content)\n",
    "        \n",
    "        if cleaned_content:\n",
    "            if file_date not in data_dict:\n",
    "                data_dict[file_date] = cleaned_content\n",
    "            else:\n",
    "                data_dict[file_date] += \" \" + cleaned_content\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ ({filename}): {e}\")\n",
    "\n",
    "# 3. ê²°ê³¼ ì €ì¥\n",
    "if data_dict:\n",
    "    df = pd.DataFrame(list(data_dict.items()), columns=['date', 'content'])\n",
    "    df = df.sort_values(by='date').reset_index(drop=True)\n",
    "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"âœ… ì‘ì—… ì™„ë£Œ! ì†Œìˆ˜ì ì´ ë³´í˜¸ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“Š ì´ í†µí•© ë‚ ì§œ ìˆ˜: {len(df)}ì¼\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8518e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eKoNLPy ë¡œë“œ ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "##n-gramì¶”ì¶œ\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# eKoNLPyê°€ ì„¤ì¹˜ëœ ê²½ë¡œë¥¼ ì§ì ‘ ì¶”ê°€\n",
    "lib_path = r'C:\\Users\\eeyy1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages'\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.append(lib_path)\n",
    "\n",
    "try:\n",
    "    from ekonlpy.sentiment import MPCK\n",
    "    print(\"âœ… eKoNLPy ë¡œë“œ ì„±ê³µ!\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"âŒ ì—¬ì „íˆ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# 1. eKoNLPyì˜ ê¸ˆìœµ í†µí™”ì •ì±… ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "mpck = MPCK()\n",
    "\n",
    "def get_sentiment_features(text):\n",
    "    # 2. í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ê¸ˆìœµ ë‹¨ì–´ ë° n-gram ì¶”ì¶œ)\n",
    "    tokens = mpck.tokenize(text)\n",
    "    \n",
    "    # 3. ë§¤íŒŒ(Hawkish)/ë¹„ë‘˜ê¸°íŒŒ(Dovish) íŒë‹¨ì„ ìœ„í•œ n-gram ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    ngram = mpck.ngramize(tokens)\n",
    "    \n",
    "    return tokens + ngram\n",
    "\n",
    "# ì˜ˆì‹œ: \"ê¸ˆë¦¬ê°€ ì¸ìƒë  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\" -> ['ê¸ˆë¦¬/NNG', 'ì¸ìƒ/NNG', 'ê¸ˆë¦¬;ì¸ìƒ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48ca868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ê¸ˆë¦¬', 'NNG'), ('ì¸ìƒ', 'NNG'), ('ê°€ëŠ¥ì„±', 'NNG'), ('ì´', 'JKS'), ('ë†’', 'VA'), ('ìŠµë‹ˆë‹¤', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from ekonlpy.tag import Mecab\n",
    "\n",
    "# MPCK ëŒ€ì‹  Mecab í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# eKoNLPyì˜ Mecabì€ ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ì´ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "mecab = Mecab()\n",
    "\n",
    "def get_tokens(text):\n",
    "    # ê¸ˆìœµ ìš©ì–´ ë‹¨ìœ„ë¡œ í† í°í™”\n",
    "    return mecab.pos(text)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_text = \"ê¸ˆë¦¬ ì¸ìƒ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\"\n",
    "print(get_tokens(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8be8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\n",
      "âœ… ì´ 2466ê°œì˜ ë¦¬í¬íŠ¸ ë¡œë“œ ì™„ë£Œ.\n",
      "ğŸš€ ë¶„ì„ ì‹œì‘ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 30~60ë¶„)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2466/2466 [13:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ’¾ ì €ì¥ ìœ„ì¹˜: C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\preprocessing\\bond_reports_preprocessing\\tokenized_reports_final.csv\n",
      "ğŸ“Š ìµœì¢… ë°ì´í„° ìˆ˜: 2466ê°œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ekonlpy.sentiment import MPCK\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (ë”°ì˜´í‘œ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ)\n",
    "csv_path = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\integrated_reports_2012_2025.csv'\n",
    "output_dir = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\preprocessing\\bond_reports_preprocessing'\n",
    "output_path = os.path.join(output_dir, 'tokenized_reports_final.csv')\n",
    "\n",
    "# ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "print(\"ğŸ“‚ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"âœ… ì´ {len(df)}ê°œì˜ ë¦¬í¬íŠ¸ ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "# 3. eKoNLPy ê¸ˆìœµ í˜•íƒœì†Œ ë¶„ì„ê¸°(MPCK) ì´ˆê¸°í™”\n",
    "# â€» ë§Œì•½ ì—¬ê¸°ì„œ UnicodeDecodeErrorê°€ ë‚œë‹¤ë©´ ì•„ê¹Œ ì•Œë ¤ë“œë¦° mpck.py ìˆ˜ì •ì„ ê¼­ í•´ì£¼ì„¸ìš”!\n",
    "mpck = MPCK()\n",
    "\n",
    "def get_tokens_and_ngrams(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return [] # ë¹ˆ ë¬¸ìì—´ ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    \n",
    "    try:\n",
    "        tokens = mpck.tokenize(text)\n",
    "        ngrams = mpck.ngramize(tokens)\n",
    "        \n",
    "        # ë¦¬ìŠ¤íŠ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”í•´ì„œ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "        result = tokens + ngrams\n",
    "        \n",
    "        return result  # [\", \".join(result) ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°˜í™˜]\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "# 4. ì „ì²´ ë°ì´í„° ë¶„ì„ ì‹œì‘\n",
    "print(f\"ğŸš€ ë¶„ì„ ì‹œì‘ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 30~60ë¶„)...\")\n",
    "tqdm.pandas() # ì§„í–‰ë°” ì„¤ì •\n",
    "\n",
    "# 'content' ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ 'tokens' ì»¬ëŸ¼ì— ì €ì¥\n",
    "df['tokens'] = df['content'].progress_apply(get_tokens_and_ngrams)\n",
    "\n",
    "# 5. ê²°ê³¼ ì €ì¥ (ë‚ ì§œì™€ í† í°í™” ê²°ê³¼ë§Œ ê¹”ë”í•˜ê²Œ ì €ì¥)\n",
    "# í›„ì— ê¸ˆë¦¬ ë°ì´í„°ì™€ ë³‘í•©í•˜ê¸° ìœ„í•´ dateì™€ tokensë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "final_save_df = df[['date', 'tokens']].copy()\n",
    "final_save_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n",
    "print(f\"ğŸ“Š ìµœì¢… ë°ì´í„° ìˆ˜: {len(final_save_df)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224a9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë…¼ë¬¸ ê·œê²© n-gram ìƒì„± ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2458/2458 [00:39<00:00, 61.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… n-gram ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ: C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\preprocessing\\bond_reports_preprocessing\\final_paper_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ekonlpy.sentiment.utils import MPTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (ë”°ì˜´í‘œ ì¤‘ë³µ ì œê±° ë²„ì „)\n",
    "base_path = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\preprocessing\\bond_reports_preprocessing'\n",
    "token_csv_path = os.path.join(base_path, 'tokenized_reports.csv')\n",
    "\n",
    "# 2. íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv(token_csv_path)\n",
    "tokenizer = MPTokenizer()\n",
    "\n",
    "def make_ngrams(tokens_str):\n",
    "    if not isinstance(tokens_str, str) or tokens_str == \"\":\n",
    "        return \"\"\n",
    "    token_list = [t.strip() for t in tokens_str.split(',')]\n",
    "    try:\n",
    "        # ë…¼ë¬¸ í•µì‹¬: ë‹¨ì–´ë“¤ì„ n-gramìœ¼ë¡œ ì¡°í•© (ì˜ˆ: ê¸ˆë¦¬/NNG;ì¸ìƒ/NNG)\n",
    "        ngrams = tokenizer.ngramize(token_list)\n",
    "        return \", \".join(token_list + ngrams)\n",
    "    except:\n",
    "        return tokens_str\n",
    "\n",
    "print(\"ğŸš€ ë…¼ë¬¸ ê·œê²© n-gram ìƒì„± ì‹œì‘...\")\n",
    "tqdm.pandas()\n",
    "df['ngram_features'] = df['tokens'].progress_apply(make_ngrams)\n",
    "\n",
    "# 3. ì¤‘ê°„ íŒŒì¼ ì €ì¥ (ì´ê²Œ ìˆì–´ì•¼ ë‹¤ìŒ ë‹¨ê³„ê°€ ëŒì•„ê°‘ë‹ˆë‹¤)\n",
    "feature_path = os.path.join(base_path, 'final_paper_features.csv')\n",
    "df.to_csv(feature_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… n-gram ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ: {feature_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc68876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ìµœì¢… íŒŒì¼: C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\preprocessing\\bond_reports_preprocessing\\final_integrated_full_v2.csv\n",
      "ğŸ“Š ì´ 2458ì¼ì¹˜ì˜ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 1. ì›ë³¸ ë¦¬í¬íŠ¸ì™€ ìœ„ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ íŒŒì¼ ê²½ë¡œ\n",
    "raw_path = r'C:\\KDT\\only-pull-me\\PJT_BOK\\team2_forecast_pjt\\crawler\\bond_reports\\integrated_reports_2012_2025.csv'\n",
    "feature_path = os.path.join(base_path, 'final_paper_features.csv')\n",
    "\n",
    "# 2. ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ í†µì¼\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "df_features = pd.read_csv(feature_path)\n",
    "\n",
    "df_raw['date'] = pd.to_datetime(df_raw['date'].astype(str)).dt.strftime('%Y-%m-%d')\n",
    "df_features['date'] = pd.to_datetime(df_features['date'].astype(str)).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (contentì™€ tokensë§Œ ë‚¨ê¸°ê¸°)\n",
    "df_raw_sub = df_raw[['date', df_raw.columns[1]]].copy() \n",
    "df_raw_sub.columns = ['date', 'content']\n",
    "\n",
    "df_features_sub = df_features[['date', 'ngram_features']].copy()\n",
    "df_features_sub.columns = ['date', 'tokens']\n",
    "\n",
    "# 4. ë‚ ì§œ ê¸°ì¤€ ë³‘í•© ë° ì¤‘ë³µ ë‚ ì§œ í•©ì¹˜ê¸°\n",
    "final_df = pd.merge(df_raw_sub, df_features_sub, on='date', how='inner')\n",
    "final_df = final_df.groupby('date').agg({\n",
    "    'content': lambda x: ' '.join(x.astype(str)),\n",
    "    'tokens': lambda x: ', '.join(x.astype(str))\n",
    "}).reset_index()\n",
    "\n",
    "# 5. ìµœì¢… í˜•ì‹ ë§ì¶”ê¸° ë° ì €ì¥\n",
    "final_df['category'] = \"ë¦¬í¬íŠ¸\"\n",
    "final_df['source'] = \"ì¦ê¶Œì‚¬\"\n",
    "final_df = final_df[['date', 'content', 'tokens', 'category', 'source']]\n",
    "\n",
    "final_output = os.path.join(base_path, 'final_integrated_full_v2.csv')\n",
    "final_df.to_csv(final_output, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ìµœì¢… íŒŒì¼: {final_output}\")\n",
    "print(f\"ğŸ“Š ì´ {len(final_df)}ì¼ì¹˜ì˜ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641358bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: True\n",
      "   ìƒ˜í”Œ ë¬¸êµ¬: ... ë°”ë¼ê¸° ì´ë¥¸ ì‹œì . 2012ë…„ 1ì›”...\n",
      "[1ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: True\n",
      "   ìƒ˜í”Œ ë¬¸êµ¬: ...13 2012. 2012ë…„ 1ì›”...\n",
      "[2ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: True\n",
      "   ìƒ˜í”Œ ë¬¸êµ¬: ...ë³´ë©° ëŒ€ì‘ í•  í•„ìš”. 2012ë…„ 1ì›”...\n",
      "[3ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: True\n",
      "   ìƒ˜í”Œ ë¬¸êµ¬: .... 2012ë…„ 1ì›”...\n",
      "[4ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: True\n",
      "   ìƒ˜í”Œ ë¬¸êµ¬: ...í€ë”ë©˜íƒˆì˜ ì˜¤ë²„ë© . 2012ë…„ 1ì›”...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ìƒìœ„ 5ê°œë§Œ)\n",
    "df_sample = pd.read_csv(final_output, nrows=5) \n",
    "\n",
    "# 2. ë§ˆì¹¨í‘œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n",
    "for i, content in enumerate(df_sample['content']):\n",
    "    has_dot = \".\" in content\n",
    "    print(f\"[{i}ë²ˆ í–‰] ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€: {has_dot}\")\n",
    "    if has_dot:\n",
    "        # ë§ˆì¹¨í‘œ ì£¼ë³€ 20ê¸€ì ì¶œë ¥í•´ì„œ í™•ì¸\n",
    "        dot_idx = content.find(\".\")\n",
    "        print(f\"   ìƒ˜í”Œ ë¬¸êµ¬: ...{content[max(0, dot_idx-10):dot_idx+10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acdd0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ë¶„ë¦¬ëœ ë¬¸ì¥ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ) ---\n",
      "ë¬¸ì¥: 13 2012.\n",
      "ë¬¸ì¥: 2012ë…„ 1ì›” ê¸ˆìœµí†µí™”ìœ„ì›íšŒ.\n",
      "ë¬¸ì¥: ì„±ì¥.\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ í˜•íƒœê°€ ì˜ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ ë” ê¸´ ìƒ˜í”Œ í™•ì¸\n",
    "sample_text = df_sample['content'].iloc[1] # 1ë²ˆ ì¸ë±ìŠ¤(ë‘ ë²ˆì§¸ ë‚ ì§œ) ë°ì´í„°\n",
    "\n",
    "# ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ëì„ ë•Œ ë¬¸ì¥ì´ ë§ì´ ë˜ëŠ”ì§€ í™•ì¸\n",
    "sentences = sample_text.split('.')\n",
    "print(\"--- ë¶„ë¦¬ëœ ë¬¸ì¥ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ) ---\")\n",
    "for sent in sentences[:3]:\n",
    "    print(f\"ë¬¸ì¥: {sent.strip()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e07302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
