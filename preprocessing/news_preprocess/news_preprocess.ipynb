{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f3f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from ekonlpy.sentiment import MPCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d726b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ekonlpy\n",
      "  Downloading ekonlpy-2.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=8.1.6 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (8.1.8)\n",
      "Collecting fugashi>=1.3.3 (from ekonlpy)\n",
      "  Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl.metadata (7.5 kB)\n",
      "Collecting mecab-ko-dic>=1.0.0 (from ekonlpy)\n",
      "  Downloading mecab-ko-dic-1.0.0.tar.gz (33.2 MB)\n",
      "     ---------------------------------------- 0.0/33.2 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.1/33.2 MB 9.0 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 12.8/33.2 MB 31.0 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 23.1/33.2 MB 36.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  32.8/33.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.2/33.2 MB 36.9 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (3.9.2)\n",
      "Collecting pandas<=2.3.2,>=1.5.3 (from ekonlpy)\n",
      "  Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy<=1.13.1,>1.10.0 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from click>=8.1.6->ekonlpy) (0.4.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<=2.3.2,>=1.5.3->ekonlpy) (1.17.0)\n",
      "Downloading ekonlpy-2.2.0-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 10.0/10.3 MB 47.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 45.6 MB/s  0:00:00\n",
      "Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.0/11.3 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 33.7 MB/s  0:00:00\n",
      "Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl (509 kB)\n",
      "Building wheels for collected packages: mecab-ko-dic\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): started\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mecab-ko-dic: filename=mecab_ko_dic-1.0.0-py3-none-any.whl size=33424458 sha256=87ba44ece365411cd8bfba948d5049e83586fef5b05a6638de0304655c25cb7f\n",
      "  Stored in directory: c:\\users\\hg432\\appdata\\local\\pip\\cache\\wheels\\1e\\26\\c0\\ed4c061096b1480abefe1e2a0356543bf7a38f61c037b69ab6\n",
      "Successfully built mecab-ko-dic\n",
      "Installing collected packages: mecab-ko-dic, fugashi, pandas, ekonlpy\n",
      "\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "  Attempting uninstall: pandas\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "    Found existing installation: pandas 2.3.3\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "    Uninstalling pandas-2.3.3:\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "      Successfully uninstalled pandas-2.3.3\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [ekonlpy]\n",
      "   ---------------------------------------- 4/4 [ekonlpy]\n",
      "\n",
      "Successfully installed ekonlpy-2.2.0 fugashi-1.5.2 mecab-ko-dic-1.0.0 pandas-2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install ekonlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0df869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_news(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # ê¸°ì ì´ë©”ì¼, ì–¸ë¡ ì‚¬ ì´ë¦„ ì œê±°\n",
    "    text = re.sub(r'\\[.+?ê¸°ì\\]|\\[.+?ë‰´ìŠ¤\\]|\\[.+?ì œê³µ\\]', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„ì œê³µ?=.+?$', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„?=.+?$', '', text)\n",
    "    \n",
    "    # í™ë³´ ë¬¸êµ¬ ì œê±°\n",
    "    text = re.sub(r'ì´\\s*ê¸°ì‚¬ëŠ”\\s*.+?\\s*ì—\\s*ê²Œì¬ëœ\\s*ê¸°ì‚¬ì…ë‹ˆë‹¤\\.', '', text)\n",
    "\n",
    "    # ì¤„ë°”ê¿ˆ ê³µë°±ìœ¼ë¡œ\n",
    "    text = text.replace('\\u2028', ' ').replace('\\u2029', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±, ë§ˆì¹¨í‘œë§Œ ë‚¨ê¸°ê¸° (ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•´ ë‚¨ê²¨ì•¼ í•¨)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s\\.]', ' ', text)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ ì¤„ì´ê¸°\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71236/71236 [00:03<00:00, 21872.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë§¤ì¼ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (71236ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89012/89012 [00:05<00:00, 17015.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë¨¸ë‹ˆíˆ¬ë°ì´ ì²˜ë¦¬ ì™„ë£Œ (89012ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79208/79208 [00:04<00:00, 19770.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” í•œêµ­ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (79208ê±´)\n",
      "\n",
      "âœ¨ ì´ 239456ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ í´ë Œì§• ë°ì´í„° í•©ì¹˜ê¸°\n",
    "tqdm.pandas()\n",
    "\n",
    "folder_path = '../../db/news_contents/news_contents_*.csv'\n",
    "file_list = glob.glob(folder_path)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in file_list:\n",
    "    filename = os.path.basename(file)\n",
    "    source_name = filename.split('_')[-1].replace('.csv', '')\n",
    "\n",
    "    df = pd.read_csv(file, usecols=['date', 'content'])\n",
    "\n",
    "    # ì˜¤ì—¼ ë°ì´í„° ì œê±°ìš©\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['date'] = pd.to_datetime(df['date'])\n",
    "    temp_df['date'] = temp_df['date'].dt.date\n",
    "\n",
    "    temp_df['content'] = df['content'].progress_apply(clean_news)\n",
    "    temp_df['tokens'] = ''\n",
    "    temp_df['category'] = 'ë‰´ìŠ¤'\n",
    "    temp_df['source'] = source_name\n",
    "\n",
    "    all_data.append(temp_df)\n",
    "    print(f\"âœ” {source_name} ì²˜ë¦¬ ì™„ë£Œ ({len(temp_df)}ê±´)\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df[['date', 'content', 'tokens', 'category', 'source']]\n",
    "    final_df.to_csv('news_preprocessed_integrated.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ¨ ì´ {len(final_df)}ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngramize(tokens, max_n=5):\n",
    "    keep_tags = ['NNG', 'VV', 'VA', 'MAG']\n",
    "    filtered = [w for w in tokens if w.split('/')[1] in keep_tags]\n",
    "    ngram_results = []\n",
    "    for pos in range(len(filtered)):\n",
    "        for n in range(1, max_n + 1):\n",
    "            if pos + n <= len(filtered):\n",
    "                ngram = \";\".join(filtered[pos : pos + n])\n",
    "                ngram_results.append(ngram)\n",
    "                \n",
    "    return ngram_results\n",
    "\n",
    "mpck = MPCK()\n",
    "\n",
    "def get_tokens(text):\n",
    "    try:\n",
    "        if not text or len(str(text)) < 10:\n",
    "            return \"\"\n",
    "        tokens = mpck.tokenize(text)\n",
    "        ngrams = ngramize(tokens)\n",
    "        return \", \".join(tokens+ngrams)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â™»ï¸ ê¸°ì¡´ì— ì‘ì—…í•˜ë˜ 'news_preprocessed_output.csv'ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: 3836ê±´ / ì „ì²´: 239456ê±´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3836/3836 [00:16<00:00, 238.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: news_preprocessed_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = 'news_preprocessed_integrated.csv'\n",
    "output_file = 'news_preprocessed_output.csv'\n",
    "batch_size = 5000 \n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"â™»ï¸ ê¸°ì¡´ì— ì‘ì—…í•˜ë˜ '{output_file}'ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(output_file)\n",
    "else:\n",
    "    print(f\"ğŸ†• ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. '{input_file}'ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    if 'tokens' not in df.columns:\n",
    "        df['tokens'] = \"\"\n",
    "\n",
    "\n",
    "to_process = df[df['tokens'].isna() | (df['tokens'] == \"\")].index\n",
    "print(f\"ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: {len(to_process)}ê±´ / ì „ì²´: {len(df)}ê±´\")\n",
    "\n",
    "try:\n",
    "    for i, idx in enumerate(tqdm(to_process)):\n",
    "        df.at[idx, 'tokens'] = get_tokens(df.at[idx, 'content'])\n",
    "        \n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(to_process):\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ ì‚¬ìš©ìê°€ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ë‚´ìš©ì€ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "df_news = pd.read_csv('news_preprocessed_output.csv', encoding='utf-8-sig')\n",
    "\n",
    "def string_to_list(text):\n",
    "    if pd.isna(text): return \"[]\" # ë¹ˆ ì¹¸ ì²˜ë¦¬\n",
    "    clean_list = [t.strip() for t in str(text).split(',') if t.strip()]\n",
    "    return json.dumps(clean_list, ensure_ascii=False)\n",
    "\n",
    "df_news['tokens'] = df_news['tokens'].apply(string_to_list)\n",
    "\n",
    "output_path = 'news_preprocessed_fixed.csv'\n",
    "df_news.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
