{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f3f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d726b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ekonlpy\n",
      "  Downloading ekonlpy-2.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=8.1.6 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (8.1.8)\n",
      "Collecting fugashi>=1.3.3 (from ekonlpy)\n",
      "  Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl.metadata (7.5 kB)\n",
      "Collecting mecab-ko-dic>=1.0.0 (from ekonlpy)\n",
      "  Downloading mecab-ko-dic-1.0.0.tar.gz (33.2 MB)\n",
      "     ---------------------------------------- 0.0/33.2 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.1/33.2 MB 9.0 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 12.8/33.2 MB 31.0 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 23.1/33.2 MB 36.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  32.8/33.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.2/33.2 MB 36.9 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (3.9.2)\n",
      "Collecting pandas<=2.3.2,>=1.5.3 (from ekonlpy)\n",
      "  Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy<=1.13.1,>1.10.0 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from click>=8.1.6->ekonlpy) (0.4.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<=2.3.2,>=1.5.3->ekonlpy) (1.17.0)\n",
      "Downloading ekonlpy-2.2.0-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 10.0/10.3 MB 47.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 45.6 MB/s  0:00:00\n",
      "Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.0/11.3 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 33.7 MB/s  0:00:00\n",
      "Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl (509 kB)\n",
      "Building wheels for collected packages: mecab-ko-dic\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): started\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mecab-ko-dic: filename=mecab_ko_dic-1.0.0-py3-none-any.whl size=33424458 sha256=87ba44ece365411cd8bfba948d5049e83586fef5b05a6638de0304655c25cb7f\n",
      "  Stored in directory: c:\\users\\hg432\\appdata\\local\\pip\\cache\\wheels\\1e\\26\\c0\\ed4c061096b1480abefe1e2a0356543bf7a38f61c037b69ab6\n",
      "Successfully built mecab-ko-dic\n",
      "Installing collected packages: mecab-ko-dic, fugashi, pandas, ekonlpy\n",
      "\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "  Attempting uninstall: pandas\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "    Found existing installation: pandas 2.3.3\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "    Uninstalling pandas-2.3.3:\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "      Successfully uninstalled pandas-2.3.3\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [ekonlpy]\n",
      "   ---------------------------------------- 4/4 [ekonlpy]\n",
      "\n",
      "Successfully installed ekonlpy-2.2.0 fugashi-1.5.2 mecab-ko-dic-1.0.0 pandas-2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install ekonlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d0df869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_news(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # ê¸°ì ì´ë©”ì¼, ì–¸ë¡ ì‚¬ ì´ë¦„ ì œê±°\n",
    "    text = re.sub(r'\\[.+?ê¸°ì\\]|\\[.+?ë‰´ìŠ¤\\]|\\[.+?ì œê³µ\\]', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„ì œê³µ?=.+?$', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„?=.+?$', '', text)\n",
    "    \n",
    "    # í™ë³´ ë¬¸êµ¬ ì œê±°\n",
    "    text = re.sub(r'ì´\\s*ê¸°ì‚¬ëŠ”\\s*.+?\\s*ì—\\s*ê²Œì¬ëœ\\s*ê¸°ì‚¬ì…ë‹ˆë‹¤\\.', '', text)\n",
    "\n",
    "    # ì¤„ë°”ê¿ˆ ê³µë°±ìœ¼ë¡œ\n",
    "    text = text.replace('\\u2028', ' ').replace('\\u2029', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±, ë§ˆì¹¨í‘œë§Œ ë‚¨ê¸°ê¸° (ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•´ ë‚¨ê²¨ì•¼ í•¨)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s\\.]', ' ', text)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ ì¤„ì´ê¸°\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822f092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71236/71236 [00:03<00:00, 21872.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë§¤ì¼ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (71236ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89012/89012 [00:05<00:00, 17015.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë¨¸ë‹ˆíˆ¬ë°ì´ ì²˜ë¦¬ ì™„ë£Œ (89012ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79208/79208 [00:04<00:00, 19770.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” í•œêµ­ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (79208ê±´)\n",
      "\n",
      "âœ¨ ì´ 239456ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ í´ë Œì§• ë°ì´í„° í•©ì¹˜ê¸°\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "folder_path = '../../db/news_contents/news_contents_*.csv'\n",
    "file_list = glob.glob(folder_path)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in file_list:\n",
    "    filename = os.path.basename(file)\n",
    "    source_name = filename.split('_')[-1].replace('.csv', '')\n",
    "\n",
    "    df = pd.read_csv(file, usecols=['date', 'content'])\n",
    "\n",
    "    # ì˜¤ì—¼ ë°ì´í„° ì œê±°ìš©\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['date'] = pd.to_datetime(df['date'])\n",
    "    temp_df['date'] = temp_df['date'].dt.date\n",
    "\n",
    "    temp_df['content'] = df['content'].progress_apply(clean_news)\n",
    "    temp_df['tokens'] = ''\n",
    "    temp_df['category'] = 'ë‰´ìŠ¤'\n",
    "    temp_df['source'] = source_name\n",
    "\n",
    "    all_data.append(temp_df)\n",
    "    print(f\"âœ” {source_name} ì²˜ë¦¬ ì™„ë£Œ ({len(temp_df)}ê±´)\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df[['date', 'content', 'tokens', 'category', 'source']]\n",
    "    final_df.to_csv('news_preprocessed_integrated.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ¨ ì´ {len(final_df)}ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. 'news_preprocessed_integrated.csv'ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
      "ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: 239456ê±´ / ì „ì²´: 239456ê±´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë°¤ìƒ˜ í† í°í™” ì‘ì—… ì¤‘:   0%|          | 0/239456 [00:00<?, ?it/s]C:\\Users\\hg432\\AppData\\Local\\Temp\\ipykernel_26596\\483549623.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'ì¤‘êµ¬/NNG, ë³¸ì /NNG, ë”œë§ë£¸/NNG, í˜„í™©/NNG, íŒ/NNG, ì½”ìŠ¤í”¼/NNG, í‘œì‹œ/NNG, ë˜/VV, ì—°ì†/NNG, ìƒìŠ¹/NNG, í•˜/VV, ì½”ìŠ¤í”¼/NNG, ê¸‰ë½/NNG, í•˜/XSV, ì„ /NNG, ê°„ì‹ íˆ/MAG, ì§€ì¼°/VV, ê°„ë°¤/NNG, íì‡„/NNG, í•´ì œ/NNG, ë¶ˆêµ¬/NNG, ê¸ˆë¦¬/NNG, ì¸í•˜/NNG, ê¸°ëŒ€/NNG, ì•½í™”/NNG, í‰ê°€/NNG, ë…¼ë€/NNG, ë¶ˆê±°ì§€/VV, ë§¤ë„/NNG, ì‹¬í™”/NNG, í•˜/XSV, ê¸‰ë½/NNG, ì˜í–¥/NNG, í’€ì´/NNG, ë˜/VV, ì½”ìŠ¤í”¼/NNG, ëŒ€ë¹„/NNG, í¬ì¸íŠ¸/NNG, ë‚´ë¦¬/VV, ê±°ë˜/NNG, ë§ˆì¹˜/VV, ì§€ìˆ˜/NNG, ëŒ€ë¹„/NNG, í¬ì¸íŠ¸/NNG, ê¸‰ë½/NNG, ì¶œë°œ/NNG, ë’¤/NNG, ë‚™í­/NNG, í™•ëŒ€/NNG, í•˜/VV, ê°„ë°¤/NNG, ì‚°ì—…/NNG, ê³ í‰ê°€/NNG, ìš°ë ¤/NNG, ë¯¸/NNG, fed/NNG, fed/NNG, fed/NNG, ë§¤íŒŒ/NNG, í†µí™”/NNG, ê¸´ì¶•/NNG, ì„ í˜¸/NNG, ë”°ë¥´/VV, ê¸ˆë¦¬ë™ê²°/NNG, ì „ë§/NNG, ê¸‰ë½/NNG, ì˜í–¥/NNG, ë¶„ì„/NNG, ë˜/VV, ì§€ìˆ˜/NNG, ëŒ€ë¹„/NNG, í¬ì¸íŠ¸/NNG, í•˜ë½/NNG, ë‚˜ìŠ¤ë‹¥/NNG, í¬ì¸íŠ¸/NNG, ë‚´ë¦¬/VV, ì¥/NNG, ë§ˆê°/NNG, í•˜/VV, ë‹¤ìš°ì¡´ìŠ¤ì‚°ì—…í‰ê· ì§€ìˆ˜/NNG, ëŒ€ë¹„/NNG, í¬ì¸íŠ¸/NNG, í•˜ë½/NNG, ê±°ë˜/NNG, ë§ˆì¹˜/VV, ë˜/MAG, í‹°ì–´/NNG, ì£¼ê°€/NNG, ê¸‰ë½/NNG, í•˜/VV, í•˜ë½/NNG, í•˜/VV, ê¸ˆë¦¬/NNG, ì¸í•˜/NNG, ê¸°ëŒ€/NNG, í›„í‡´/NNG, í•˜/XSV, ì„±ì¥/NNG, ë¶€ë‹´/NNG, ì¦ê°€/NNG, í–ˆë‹¤/XSV, ê¸°ì—…/NNG, ê±°í’ˆ/NNG, ìš°ë ¤/NNG, ë”/MAG, í•´ì§€/NNG, ì¤‘ì‹¬/NNG, ê¸€ë¡œë²Œ/NNG, ì¦ì‹œ/NNG, í•˜ë½/NNG, ëšœë ·/NNG, í•˜/VV, ì™¸êµ­ì¸/NNG, ê¸°ê´€/NNG, ìˆœë§¤ë„/NNG, í•˜/VV, ë°˜ë©´/NNG, ê°œì¸/NNG, í™€ë¡œ/MAG, ìˆœë§¤ìˆ˜/NNG, í•˜/VV, ì—…ì¢…ë³„/NNG, ë³´í—˜/NNG, ì •ë°€/NNG, ì¦ê¶Œ/NNG, ë‚´ë¦¬/VV, ë°˜ë©´/NNG, ì œì•½/NNG, ìƒìŠ¹/NNG, í•˜/VV, ì½”ìŠ¤í”¼/NNG, ì‹œê°€ì´ì•¡/NNG, ìƒìœ„/NNG, ì¢…ëª©/NNG, í•˜ë½/NNG, í•˜/VV, ì£¼ê°€/NNG, ê¸‰ë½/NNG, í–ˆìœ¼ë©°/XSV, ë¹Œë¦¬/VV, ì–´ë¡œ/NNG, ìŠ¤í˜ì´ìŠ¤/NNG, ê¸°ì•„/NNG, ì•½ì„¸/NNG, ë§ˆê°/NNG, í•˜/VV, ë°˜ë©´/NNG, í•œë¯¸/NNG, ê´€ì„¸/NNG, ì•ˆë³´/NNG, í˜‘ìƒ/NNG, ê²°ê³¼ë¬¼/NNG, ì¡°ì¸íŠ¸/NNG, íŒ©íŠ¸/NNG, ì„¤ëª…/NNG, ìë£Œ/NNG, ë°œí‘œ/NNG, ê°•ì„¸/NNG, ë³´/VV, ì½”ìŠ¤ë‹¥/NNG, ì§€ìˆ˜/NNG, ëŒ€ë¹„/NNG, í¬ì¸íŠ¸/NNG, ë‚´ë¦¬/VV, ì¥/NNG, ë§ˆê°/NNG, í•˜/VV, ì½”ìŠ¤ë‹¥/NNG, ì‹œì¥/NNG, ì™¸êµ­ì¸/NNG, ê¸°ê´€/NNG, ìˆœë§¤ë„/NNG, ë°˜ë©´/NNG, ê°œì¸/NNG, ìˆœë§¤ìˆ˜/NNG, í•˜/VV, ì½”ìŠ¤ë‹¥/NNG, ì‹œê°€ì´ì•¡/NNG, ì¢…ëª©/NNG, ë“±ë½/NNG, ì—‡ê°ˆë¦¬/VV, ë¡œë³´í‹±ìŠ¤/NNG, í•˜ë½/NNG, í•˜/VV, ë°˜ë©´/NNG, ì—ì´/NNG, ë¹„ì—˜/NNG, íŒŒë§ˆ/NNG, ìƒìŠ¹/NNG, í•˜/VV, ì™¸í™˜ì‹œì¥/NNG, ë‹¬ëŸ¬/NNG, ì›í™”/NNG, ëŒ€ë¹„/NNG, ì˜¤ë¥´/VV, ê±°ë˜/NNG, ë§ˆì¹˜/VV, fed/NNG;ë§¤íŒŒ/NNG;í†µí™”/NNG;ê¸´ì¶•/NNG;ì„ í˜¸/NNG, ê¸ˆë¦¬/NNG;ì¸í•˜/NNG;ê¸°ëŒ€/NNG;ì•½í™”/NNG, ê¸ˆë¦¬/NNG;ì¸í•˜/NNG;ê¸°ëŒ€/NNG;í›„í‡´/NNG, ë¯¸/NNG;fed/NNG;ë§¤íŒŒ/NNG, ê³ í‰ê°€/NNG;ìš°ë ¤/NNG, ì „ë§/NNG;ê¸‰ë½/NNG, ì„±ì¥/NNG;ë¶€ë‹´/NNG, ê±°í’ˆ/NNG;ìš°ë ¤/NNG' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, 'tokens'] = get_tokens(df.at[idx, 'content'])\n",
      "ë°¤ìƒ˜ í† í°í™” ì‘ì—… ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 239456/239456 [55:18<00:00, 72.17it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: news_preprocessed_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ekonlpy.sentiment import MPCK\n",
    "\n",
    "input_file = 'news_preprocessed_integrated.csv'\n",
    "output_file = 'news_preprocessed_output.csv'\n",
    "batch_size = 5000 \n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"â™»ï¸ ê¸°ì¡´ì— ì‘ì—…í•˜ë˜ '{output_file}'ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(output_file)\n",
    "else:\n",
    "    print(f\"ğŸ†• ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. '{input_file}'ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    if 'tokens' not in df.columns:\n",
    "        df['tokens'] = \"\"\n",
    "\n",
    "mpck = MPCK()\n",
    "\n",
    "def get_tokens(text):\n",
    "    try:\n",
    "        if not text or len(str(text)) < 10:\n",
    "            return \"\"\n",
    "        tokens = mpck.tokenize(text)\n",
    "        ngrams = mpck.ngramize(tokens)\n",
    "        return \", \".join(tokens + ngrams)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "to_process = df[df['tokens'].isna() | (df['tokens'] == \"\")].index\n",
    "print(f\"ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: {len(to_process)}ê±´ / ì „ì²´: {len(df)}ê±´\")\n",
    "\n",
    "try:\n",
    "    for i, idx in enumerate(tqdm(to_process)):\n",
    "        df.at[idx, 'tokens'] = get_tokens(df.at[idx, 'content'])\n",
    "        \n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(to_process):\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ ì‚¬ìš©ìê°€ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ë‚´ìš©ì€ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d752969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_based_ngramize(tokens, max_n=5):\n",
    "    # ë…¼ë¬¸ 4ëŒ€ í’ˆì‚¬ + ì‚¬ìš©ìë‹˜ì˜ 'ë¶€ì •ì–´(VX)' ì¶”ê°€\n",
    "    keep_tags = ['NNG', 'VV', 'VA', 'MAG', 'VX']\n",
    "    \n",
    "    # 1. í’ˆì‚¬ í•„í„°ë§\n",
    "    filtered = [w for w in tokens if w.split('/')[1] in keep_tags]\n",
    "    \n",
    "    ngram_results = []\n",
    "    # 2. 1-gramë¶€í„° 5-gramê¹Œì§€ ìƒì„± (ë…¼ë¬¸ ë°©ì‹)\n",
    "    for pos in range(len(filtered)):\n",
    "        for n in range(1, max_n + 1):\n",
    "            if pos + n <= len(filtered):\n",
    "                ngram = \";\".join(filtered[pos : pos + n])\n",
    "                ngram_results.append(ngram)\n",
    "                \n",
    "    return ngram_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00d0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
