{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f3f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from ekonlpy.sentiment import MPCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d726b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ekonlpy\n",
      "  Downloading ekonlpy-2.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=8.1.6 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (8.1.8)\n",
      "Collecting fugashi>=1.3.3 (from ekonlpy)\n",
      "  Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl.metadata (7.5 kB)\n",
      "Collecting mecab-ko-dic>=1.0.0 (from ekonlpy)\n",
      "  Downloading mecab-ko-dic-1.0.0.tar.gz (33.2 MB)\n",
      "     ---------------------------------------- 0.0/33.2 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.1/33.2 MB 9.0 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 12.8/33.2 MB 31.0 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 23.1/33.2 MB 36.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  32.8/33.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.2/33.2 MB 36.9 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (3.9.2)\n",
      "Collecting pandas<=2.3.2,>=1.5.3 (from ekonlpy)\n",
      "  Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy<=1.13.1,>1.10.0 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from ekonlpy) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from pandas<=2.3.2,>=1.5.3->ekonlpy) (2025.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from click>=8.1.6->ekonlpy) (0.4.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from nltk>=3.8.1->ekonlpy) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hg432\\anaconda3\\envs\\nlp_study\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<=2.3.2,>=1.5.3->ekonlpy) (1.17.0)\n",
      "Downloading ekonlpy-2.2.0-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 10.0/10.3 MB 47.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 45.6 MB/s  0:00:00\n",
      "Downloading pandas-2.3.2-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.0/11.3 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 33.7 MB/s  0:00:00\n",
      "Downloading fugashi-1.5.2-cp39-cp39-win_amd64.whl (509 kB)\n",
      "Building wheels for collected packages: mecab-ko-dic\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): started\n",
      "  Building wheel for mecab-ko-dic (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mecab-ko-dic: filename=mecab_ko_dic-1.0.0-py3-none-any.whl size=33424458 sha256=87ba44ece365411cd8bfba948d5049e83586fef5b05a6638de0304655c25cb7f\n",
      "  Stored in directory: c:\\users\\hg432\\appdata\\local\\pip\\cache\\wheels\\1e\\26\\c0\\ed4c061096b1480abefe1e2a0356543bf7a38f61c037b69ab6\n",
      "Successfully built mecab-ko-dic\n",
      "Installing collected packages: mecab-ko-dic, fugashi, pandas, ekonlpy\n",
      "\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "  Attempting uninstall: pandas\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "    Found existing installation: pandas 2.3.3\n",
      "   ---------------------------------------- 0/4 [mecab-ko-dic]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "    Uninstalling pandas-2.3.3:\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "      Successfully uninstalled pandas-2.3.3\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [ekonlpy]\n",
      "   ---------------------------------------- 4/4 [ekonlpy]\n",
      "\n",
      "Successfully installed ekonlpy-2.2.0 fugashi-1.5.2 mecab-ko-dic-1.0.0 pandas-2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\hg432\\anaconda3\\envs\\nlp_study\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install ekonlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0df869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_news(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # ê¸°ì ì´ë©”ì¼, ì–¸ë¡ ì‚¬ ì´ë¦„ ì œê±°\n",
    "    text = re.sub(r'\\[.+?ê¸°ì\\]|\\[.+?ë‰´ìŠ¤\\]|\\[.+?ì œê³µ\\]', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„ì œê³µ?=.+?$', '', text)\n",
    "    text = re.sub(r'\\/ì‚¬ì§„?=.+?$', '', text)\n",
    "    \n",
    "    # í™ë³´ ë¬¸êµ¬ ì œê±°\n",
    "    text = re.sub(r'ì´\\s*ê¸°ì‚¬ëŠ”\\s*.+?\\s*ì—\\s*ê²Œì¬ëœ\\s*ê¸°ì‚¬ì…ë‹ˆë‹¤\\.', '', text)\n",
    "\n",
    "    # ì¤„ë°”ê¿ˆ ê³µë°±ìœ¼ë¡œ\n",
    "    text = text.replace('\\u2028', ' ').replace('\\u2029', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±, ë§ˆì¹¨í‘œë§Œ ë‚¨ê¸°ê¸° (ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•´ ë‚¨ê²¨ì•¼ í•¨)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s\\.]', ' ', text)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ ì¤„ì´ê¸°\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71236/71236 [00:03<00:00, 21872.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë§¤ì¼ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (71236ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89012/89012 [00:05<00:00, 17015.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ë¨¸ë‹ˆíˆ¬ë°ì´ ì²˜ë¦¬ ì™„ë£Œ (89012ê±´)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79208/79208 [00:04<00:00, 19770.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” í•œêµ­ê²½ì œ ì²˜ë¦¬ ì™„ë£Œ (79208ê±´)\n",
      "\n",
      "âœ¨ ì´ 239456ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ í´ë Œì§• ë°ì´í„° í•©ì¹˜ê¸°\n",
    "tqdm.pandas()\n",
    "\n",
    "folder_path = '../../db/news_contents/news_contents_*.csv'\n",
    "file_list = glob.glob(folder_path)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in file_list:\n",
    "    filename = os.path.basename(file)\n",
    "    source_name = filename.split('_')[-1].replace('.csv', '')\n",
    "\n",
    "    df = pd.read_csv(file, usecols=['date', 'content'])\n",
    "\n",
    "    # ì˜¤ì—¼ ë°ì´í„° ì œê±°ìš©\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['date'] = pd.to_datetime(df['date'])\n",
    "    temp_df['date'] = temp_df['date'].dt.date\n",
    "\n",
    "    temp_df['content'] = df['content'].progress_apply(clean_news)\n",
    "    temp_df['tokens'] = ''\n",
    "    temp_df['category'] = 'ë‰´ìŠ¤'\n",
    "    temp_df['source'] = source_name\n",
    "\n",
    "    all_data.append(temp_df)\n",
    "    print(f\"âœ” {source_name} ì²˜ë¦¬ ì™„ë£Œ ({len(temp_df)}ê±´)\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df[['date', 'content', 'tokens', 'category', 'source']]\n",
    "    final_df.to_csv('news_preprocessed_integrated.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ¨ ì´ {len(final_df)}ê±´ì˜ 'ë‰´ìŠ¤' ë°ì´í„° ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngramize(tokens, max_n=5):\n",
    "    keep_tags = ['NNG', 'VV', 'VA', 'MAG']\n",
    "    filtered = [w for w in tokens if w.split('/')[1] in keep_tags]\n",
    "    ngram_results = []\n",
    "    for pos in range(len(filtered)):\n",
    "        for n in range(1, max_n + 1):\n",
    "            if pos + n <= len(filtered):\n",
    "                ngram = \";\".join(filtered[pos : pos + n])\n",
    "                ngram_results.append(ngram)\n",
    "                \n",
    "    return ngram_results\n",
    "\n",
    "mpck = MPCK()\n",
    "\n",
    "def get_tokens(text):\n",
    "    try:\n",
    "        if not text or len(str(text)) < 10:\n",
    "            return \"\"\n",
    "        tokens = mpck.tokenize(text)\n",
    "        ngrams = ngramize(tokens)\n",
    "        return \", \".join(tokens+ngrams)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â™»ï¸ ê¸°ì¡´ì— ì‘ì—…í•˜ë˜ 'news_preprocessed_output.csv'ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: 3836ê±´ / ì „ì²´: 239456ê±´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3836/3836 [00:16<00:00, 238.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: news_preprocessed_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = 'news_preprocessed_integrated.csv'\n",
    "output_file = 'news_preprocessed_output.csv'\n",
    "batch_size = 5000 \n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"â™»ï¸ ê¸°ì¡´ì— ì‘ì—…í•˜ë˜ '{output_file}'ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(output_file)\n",
    "else:\n",
    "    print(f\"ğŸ†• ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. '{input_file}'ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    if 'tokens' not in df.columns:\n",
    "        df['tokens'] = \"\"\n",
    "\n",
    "\n",
    "to_process = df[df['tokens'].isna() | (df['tokens'] == \"\")].index\n",
    "print(f\"ğŸš€ ë‚¨ì€ ì‘ì—…ëŸ‰: {len(to_process)}ê±´ / ì „ì²´: {len(df)}ê±´\")\n",
    "\n",
    "try:\n",
    "    for i, idx in enumerate(tqdm(to_process)):\n",
    "        df.at[idx, 'tokens'] = get_tokens(df.at[idx, 'content'])\n",
    "        \n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(to_process):\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ ì‚¬ìš©ìê°€ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ë‚´ìš©ì€ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ ê¸°ì‚¬ ë°ì´í„° ìˆ˜ì • ì™„ë£Œ! ì´ì œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì…ë‹ˆë‹¤: news_data_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "df_news = pd.read_csv('news_preprocessed_output.csv', encoding='utf-8-sig')\n",
    "\n",
    "def string_to_list(text):\n",
    "    if pd.isna(text): return \"[]\" # ë¹ˆ ì¹¸ ì²˜ë¦¬\n",
    "    clean_list = [t.strip() for t in str(text).split(',') if t.strip()]\n",
    "    return json.dumps(clean_list, ensure_ascii=False)\n",
    "\n",
    "df_news['tokens'] = df_news['tokens'].apply(string_to_list)\n",
    "\n",
    "output_path = 'news_preprocessed_fixed.csv'\n",
    "df_news.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"âœ¨ ê¸°ì‚¬ ë°ì´í„° ìˆ˜ì • ì™„ë£Œ! ì´ì œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì…ë‹ˆë‹¤: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d5b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>14ì¼ ì„œìš¸ ì¤‘êµ¬ í•˜ë‚˜ì€í–‰ ë³¸ì  ë”œë§ë£¸ í˜„í™©íŒì— ì½”ìŠ¤í”¼ê°€ í‘œì‹œë¼ ìˆë‹¤. ë‚˜í˜ ì—°ì† ...</td>\n",
       "      <td>[\"ì¤‘êµ¬/NNG\", \"ë³¸ì /NNG\", \"ë”œë§ë£¸/NNG\", \"í˜„í™©/NNG\", \"íŒ/N...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>NHë†í˜‘ì€í–‰ì´ ìˆ˜íƒ ì¤‘ì¸ ì „êµ­ ê´‘ì—­ìì¹˜ë‹¨ì²´ ê¸ˆê³ ì˜ ì˜ˆì¹˜ì•¡ì— ëŒ€í•œ ê¸ˆë¦¬ê°€ ìµœëŒ€ 1.2...</td>\n",
       "      <td>[\"ìˆ˜íƒ/NNG\", \"ì¤‘/NNG\", \"ì „êµ­/NNG\", \"ê¸ˆê³ /NNG\", \"ì˜ˆì¹˜/NN...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>ìœ ëŸ½ì¤‘ì•™ì€í–‰ ECB ì´ 18ì¼ í˜„ì§€ì‹œê°„ ì˜ˆê¸ˆê¸ˆë¦¬ë¥¼ ë¹„ë¡¯í•œ 3ëŒ€ ì •ì±…ê¸ˆë¦¬ë¥¼ ëª¨ë‘ ë™ê²°...</td>\n",
       "      <td>[\"ecb/NNG\", \"ecb/NNG\", \"í˜„ì§€/NNG\", \"ì˜ˆê¸ˆ/NNG\", \"ê¸ˆë¦¬...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>ì€í–‰ì±„ ê¸ˆë¦¬ìƒìŠ¹ì— ì¡°ë‹¬ë¹„ìš© ëŒ€í˜•ì¦ê¶Œì‚¬ IMA ì¶œì‹œë„ ë³€ìˆ˜ ì¦ê¶Œì‚¬ ë¨¸ë‹ˆë¬´ë¸Œ ì°¨ë‹¨ ëª©ì ...</td>\n",
       "      <td>[\"ì€í–‰ì±„/NNG\", \"ê¸ˆë¦¬/NNG\", \"ìƒìŠ¹/NNG\", \"ì¡°ë‹¬/NNG\", \"ë¹„ìš©/...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-22</td>\n",
       "      <td>ë¹„íŠ¸ì½”ì¸ ê¸‰ë½ì— ìŠ¤íŠ¸ë ˆí‹°ì§€ë„ 60 í­ë½ MSCI ë‚˜ìŠ¤ë‹¥100ë“± ë²¤ì¹˜ë§ˆí¬ ì œì™¸ê°€ëŠ¥ì„± ...</td>\n",
       "      <td>[\"ë¹„íŠ¸ì½”ì¸/NNG\", \"ê¸‰ë½/NNG\", \"í•˜ë½/NNG\", \"msci/NNG\", \"...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>AI ì‹œëŒ€ ìë³¸ë„ ë¹ ë¥´ê²Œ ì´ë™ ê³ í™˜ìœ¨ ë¶€ë™ì‚° ì€í‡´ ìì‚° ì£¼ëª© R E D H O R ...</td>\n",
       "      <td>[\"ì‹œëŒ€/NNG\", \"ìë³¸/NNG\", \"ë¹ ë¥´/VA\", \"ì´ë™/NNG\", \"ê³ í™˜/NN...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-16</td>\n",
       "      <td>ì €ì›ê°€ì„± ì˜ˆê¸ˆ í™•ë³´ ìœ„í•œ ìˆ˜ë‹¨ ë¸Œëœë“œ ì¬ì •ë¹„í•˜ê³  í˜œíƒ í™•ëŒ€ 4ëŒ€ ì‹œì¤‘ì€í–‰ì´ ë¸Œëœë“œë¥¼...</td>\n",
       "      <td>[\"ì›ê°€/NNG\", \"ì˜ˆê¸ˆ/NNG\", \"í™•ë³´/NNG\", \"ìœ„í•˜/VV\", \"ë¸Œëœë“œ/N...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>ë‚´ë…„ 3ê¸° ì‚¬ì—… ì‹œì‘ ì•ë‘ê³  ì‹ ê·œì„œë¹„ìŠ¤ ë””ìì¸ ê²½ìŸ ì‹ í•œ í•˜ë‚˜ IBKê¸°ì—…ì€í–‰ì´ ë‚˜ë¼...</td>\n",
       "      <td>[\"ì‚¬ì—…/NNG\", \"ì‹œì‘/NNG\", \"ì•ë‘/VV\", \"ì‹ ê·œ/NNG\", \"ì„œë¹„ìŠ¤/N...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>ì™¸í™˜ë‹¹êµ­ ì„±íƒ„ì „ì•¼ í™˜ìœ¨ë‹¨ì† 1992ë…„ ë¸”ë™ì›¬ì¦ˆë°ì´ì—” ì¤‘ì•™ì€í–‰ë„ í€ë“œì— êµ´ë³µ ì„±ì¥ì •ì±…...</td>\n",
       "      <td>[\"ì™¸í™˜ë‹¹êµ­/NNG\", \"íƒ„ì „/NNG\", \"í™˜ìœ¨/NNG\", \"ë‹¨ì†/NNG\", \"ë¸”ë™...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>ë§¤ê²½í…ŒìŠ¤íŠ¸ ì˜ˆì œ ê¸ˆë¦¬ì— ê´€í•œ ë‹¤ìŒì˜ ì„¤ëª… ì¤‘ ì˜¬ë°”ë¥¸ ê²ƒì€. ë‹¨ë¦¬ëŠ” ë³µë¦¬ë³´ë‹¤ ë§ì€ ì´...</td>\n",
       "      <td>[\"í…ŒìŠ¤íŠ¸/NNG\", \"ì˜ˆì œ/NNG\", \"ê¸ˆë¦¬/NNG\", \"ê´€í•˜/VV\", \"ì„¤ëª…/N...</td>\n",
       "      <td>ë‰´ìŠ¤</td>\n",
       "      <td>ë§¤ì¼ê²½ì œ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            content  \\\n",
       "0  2025-11-14  14ì¼ ì„œìš¸ ì¤‘êµ¬ í•˜ë‚˜ì€í–‰ ë³¸ì  ë”œë§ë£¸ í˜„í™©íŒì— ì½”ìŠ¤í”¼ê°€ í‘œì‹œë¼ ìˆë‹¤. ë‚˜í˜ ì—°ì† ...   \n",
       "1  2025-10-21  NHë†í˜‘ì€í–‰ì´ ìˆ˜íƒ ì¤‘ì¸ ì „êµ­ ê´‘ì—­ìì¹˜ë‹¨ì²´ ê¸ˆê³ ì˜ ì˜ˆì¹˜ì•¡ì— ëŒ€í•œ ê¸ˆë¦¬ê°€ ìµœëŒ€ 1.2...   \n",
       "2  2025-12-18  ìœ ëŸ½ì¤‘ì•™ì€í–‰ ECB ì´ 18ì¼ í˜„ì§€ì‹œê°„ ì˜ˆê¸ˆê¸ˆë¦¬ë¥¼ ë¹„ë¡¯í•œ 3ëŒ€ ì •ì±…ê¸ˆë¦¬ë¥¼ ëª¨ë‘ ë™ê²°...   \n",
       "3  2025-12-14  ì€í–‰ì±„ ê¸ˆë¦¬ìƒìŠ¹ì— ì¡°ë‹¬ë¹„ìš© ëŒ€í˜•ì¦ê¶Œì‚¬ IMA ì¶œì‹œë„ ë³€ìˆ˜ ì¦ê¶Œì‚¬ ë¨¸ë‹ˆë¬´ë¸Œ ì°¨ë‹¨ ëª©ì ...   \n",
       "4  2025-11-22  ë¹„íŠ¸ì½”ì¸ ê¸‰ë½ì— ìŠ¤íŠ¸ë ˆí‹°ì§€ë„ 60 í­ë½ MSCI ë‚˜ìŠ¤ë‹¥100ë“± ë²¤ì¹˜ë§ˆí¬ ì œì™¸ê°€ëŠ¥ì„± ...   \n",
       "5  2025-12-30  AI ì‹œëŒ€ ìë³¸ë„ ë¹ ë¥´ê²Œ ì´ë™ ê³ í™˜ìœ¨ ë¶€ë™ì‚° ì€í‡´ ìì‚° ì£¼ëª© R E D H O R ...   \n",
       "6  2025-11-16  ì €ì›ê°€ì„± ì˜ˆê¸ˆ í™•ë³´ ìœ„í•œ ìˆ˜ë‹¨ ë¸Œëœë“œ ì¬ì •ë¹„í•˜ê³  í˜œíƒ í™•ëŒ€ 4ëŒ€ ì‹œì¤‘ì€í–‰ì´ ë¸Œëœë“œë¥¼...   \n",
       "7  2025-11-07  ë‚´ë…„ 3ê¸° ì‚¬ì—… ì‹œì‘ ì•ë‘ê³  ì‹ ê·œì„œë¹„ìŠ¤ ë””ìì¸ ê²½ìŸ ì‹ í•œ í•˜ë‚˜ IBKê¸°ì—…ì€í–‰ì´ ë‚˜ë¼...   \n",
       "8  2025-12-30  ì™¸í™˜ë‹¹êµ­ ì„±íƒ„ì „ì•¼ í™˜ìœ¨ë‹¨ì† 1992ë…„ ë¸”ë™ì›¬ì¦ˆë°ì´ì—” ì¤‘ì•™ì€í–‰ë„ í€ë“œì— êµ´ë³µ ì„±ì¥ì •ì±…...   \n",
       "9  2012-01-26  ë§¤ê²½í…ŒìŠ¤íŠ¸ ì˜ˆì œ ê¸ˆë¦¬ì— ê´€í•œ ë‹¤ìŒì˜ ì„¤ëª… ì¤‘ ì˜¬ë°”ë¥¸ ê²ƒì€. ë‹¨ë¦¬ëŠ” ë³µë¦¬ë³´ë‹¤ ë§ì€ ì´...   \n",
       "\n",
       "                                              tokens category source  \n",
       "0  [\"ì¤‘êµ¬/NNG\", \"ë³¸ì /NNG\", \"ë”œë§ë£¸/NNG\", \"í˜„í™©/NNG\", \"íŒ/N...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "1  [\"ìˆ˜íƒ/NNG\", \"ì¤‘/NNG\", \"ì „êµ­/NNG\", \"ê¸ˆê³ /NNG\", \"ì˜ˆì¹˜/NN...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "2  [\"ecb/NNG\", \"ecb/NNG\", \"í˜„ì§€/NNG\", \"ì˜ˆê¸ˆ/NNG\", \"ê¸ˆë¦¬...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "3  [\"ì€í–‰ì±„/NNG\", \"ê¸ˆë¦¬/NNG\", \"ìƒìŠ¹/NNG\", \"ì¡°ë‹¬/NNG\", \"ë¹„ìš©/...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "4  [\"ë¹„íŠ¸ì½”ì¸/NNG\", \"ê¸‰ë½/NNG\", \"í•˜ë½/NNG\", \"msci/NNG\", \"...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "5  [\"ì‹œëŒ€/NNG\", \"ìë³¸/NNG\", \"ë¹ ë¥´/VA\", \"ì´ë™/NNG\", \"ê³ í™˜/NN...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "6  [\"ì›ê°€/NNG\", \"ì˜ˆê¸ˆ/NNG\", \"í™•ë³´/NNG\", \"ìœ„í•˜/VV\", \"ë¸Œëœë“œ/N...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "7  [\"ì‚¬ì—…/NNG\", \"ì‹œì‘/NNG\", \"ì•ë‘/VV\", \"ì‹ ê·œ/NNG\", \"ì„œë¹„ìŠ¤/N...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "8  [\"ì™¸í™˜ë‹¹êµ­/NNG\", \"íƒ„ì „/NNG\", \"í™˜ìœ¨/NNG\", \"ë‹¨ì†/NNG\", \"ë¸”ë™...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  \n",
       "9  [\"í…ŒìŠ¤íŠ¸/NNG\", \"ì˜ˆì œ/NNG\", \"ê¸ˆë¦¬/NNG\", \"ê´€í•˜/VV\", \"ì„¤ëª…/N...       ë‰´ìŠ¤   ë§¤ì¼ê²½ì œ  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
