import pandas as pd
import os
import numpy as np
from ekonlpy.sentiment import MPCK
from multiprocessing import Pool, cpu_count

# --- [1. ngramize í•¨ìˆ˜] ---
def ngramize(tokens, max_n=5):
    keep_tags = ['NNG', 'VA', 'VAX', 'MAG', 'VV']
    filtered = [w for w in tokens if w.split('/')[1] in keep_tags]
    all_ngrams = []
    for pos in range(len(filtered)):
        for n in range(1, max_n + 1):
            if pos + n <= len(filtered):
                ngram = ";".join(filtered[pos : pos + n])
                all_ngrams.append({'ngram': ngram, 'start': pos, 'end': pos + n, 'len': n})   
    final_ngrams = []
    sorted_ngrams = sorted(all_ngrams, key=lambda x: x['len'], reverse=True)
    covered_ranges = set()
    for ngram in sorted_ngrams:
        is_covered = False
        for i in range(ngram['start'], ngram['end']):
            if i in covered_ranges:
                is_covered = True
                break
        if not is_covered:
            final_ngrams.append(ngram['ngram'])
            for i in range(ngram['start'], ngram['end']):
                covered_ranges.add(i)
    return final_ngrams

# --- [2. ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ê°œë³„ ì¼ê¾¼(Worker) í•¨ìˆ˜] ---
def worker_task(text_list):
    mpck = MPCK() 
    batch_results = []
    for text in text_list:
        try:
            tokens = mpck.tokenize(text)
            final = ngramize(tokens, max_n=5)
            batch_results.append(final)
        except Exception as e:
            print(f"ì—ëŸ¬ ë°œê²¬: {e}")
            raise e
    return batch_results

# --- [3. ë©”ì¸ ì‹¤í–‰ ì œì–´ê¸°] ---
def run_production(df, output_folder='./processed_batches', batch_size=2000):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    num_cores = 8
    total_batches = int(np.ceil(len(df) / batch_size))
    
    print(f"âš™ï¸ ì´ {len(df)}ê±´ ë°ì´í„°ë¥¼ {num_cores}ê°œ ì½”ì–´ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for i in tqdm(range(total_batches), desc="Processing Batches"):
        batch_file = os.path.join(output_folder, f"batch_{i}.parquet")
        
        # [ì²´í¬í¬ì¸íŠ¸] ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°! (ë§˜ íŽ¸í•˜ê²Œ ìž¬ì‹¤í–‰ ê°€ëŠ¥)
        if os.path.exists(batch_file):
            continue
            
        start = i * batch_size
        end = min((i + 1) * batch_size, len(df))
        chunk = df.iloc[start:end].copy()
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘
        with Pool(num_cores) as pool:
            # ë°ì´í„°ë¥¼ ì½”ì–´ ê°œìˆ˜ë§Œí¼ ë‹¤ì‹œ ìª¼ê°œì„œ ë¶„ë°°
            split_chunks = np.array_split(chunk['content'], num_cores)
            results = pool.map(worker_task, split_chunks)
            
            # ìª¼ê°œì§„ ê²°ê³¼ í•©ì³ì„œ ì»¬ëŸ¼ì— ë„£ê¸°
            flat_results = [item for sublist in results for item in sublist]
            chunk['tokens'] = flat_results
            
            # Parquet í˜•ì‹ìœ¼ë¡œ ì €ìž¥ (csvë³´ë‹¤ ë¹ ë¥´ê³  ìš©ëŸ‰ì´ ìž‘ìŒ)
            chunk.to_parquet(batch_file)

# --- [4. ì „ì²´ ì‹¤í–‰ ë¡œì§] ---
if __name__ == "__main__":
    import kss
    from tqdm import tqdm
    tqdm.pandas()

    SENTENCE_FILE = 'df_sentences.parquet'
    if os.path.exists(SENTENCE_FILE):
        print(f"âœ… ì´ë¯¸ ìª¼ê°œì§„ íŒŒì¼({SENTENCE_FILE})ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë¡œë“œ ì¤‘...")
        df_sentences = pd.read_parquet(SENTENCE_FILE)
    else:
        # 1. ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ ì›ë³¸ CSVë“¤ì„ ì½ì–´ì˜µë‹ˆë‹¤)
        print("ðŸ“‚ ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í•©ì¹˜ëŠ” ì¤‘...")
        news = pd.read_csv('../db/preprocessing/news_preprocessed_fixed.csv', encoding='utf-8')
        meetings = pd.read_csv('../db/preprocessing/meeting_preprocessed_fixed.csv', encoding='utf-8')
        reports = pd.read_csv('../db/preprocessing/final_integrated_full_v2.csv', encoding='utf-8')
        press = pd.read_csv('../db/preprocessing/press_preprocessed_fixed.csv', encoding='utf-8')

        df_total = pd.concat([news, meetings, reports, press], ignore_index=True)
        # 3. ë¬¸ì„œ ê³ ìœ  Index
        df_total['doc_id'] = df_total.index
        final_cols = ['date', 'content', 'tokens', 'category', 'source', 'doc_id']
        df_total = df_total[final_cols]
        df_total = df_total.dropna(subset=['content'])

        # 2. ë¬¸ìž¥ ë¶„ë¦¬ ìž‘ì—… (KSSëŠ” ì—¬ê¸°ì„œ ë¯¸ë¦¬ ìˆ˜í–‰)
        print("âœ‚ï¸ ë¬¸ìž¥ ë¶„ë¦¬(KSS)ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
        df_working = df_total.copy()
        df_working['content'] = df_working['content'].progress_apply(kss.split_sentences)
        df_sentences = df_working.explode('content').reset_index(drop=True)

        del df_total
        del df_working

        df_sentences['tokens'] = None
        output_columns = ['doc_id', 'date', 'content', 'tokens', 'category', 'source']
        df_sentences = df_sentences[output_columns]
        
        print(f"ðŸ’¾ ìª¼ê°œì§„ ë°ì´í„°ë¥¼ {SENTENCE_FILE}ë¡œ ì €ìž¥í•©ë‹ˆë‹¤...")
        df_sentences.to_parquet(SENTENCE_FILE)

    # 3. ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰
    run_production(df_sentences)
    
    print("âœ¨ ëª¨ë“  ìž‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! './processed_batches' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")